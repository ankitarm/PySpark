{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwpkhMmw8DZmXaIkwK4+of",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/PySpark/blob/main/Basic_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Merge two data frames"
      ],
      "metadata": {
        "id": "EbeoyZWPCR5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum"
      ],
      "metadata": {
        "id": "LbRPCwrVCKMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Merge\").getOrCreate()\n",
        "\n",
        "simpleData1 = [(1,\"name1\",\"cse\"),(2, \"name2\",\"extc\")]\n",
        "simpleschema1 = [\"ID\",\"NAME\",\"DEPT\"]\n",
        "\n",
        "simpleData2 = [(1,\"name1\",\"cse\"),(2, \"name2\",\"extc\")]\n",
        "simpleschema2 = [\"ID\",\"NAME\",\"DEPT\"]\n",
        "\n",
        "simpleDataframe1 = spark.createDataFrame(data = simpleData1, schema = simpleschema1)\n",
        "simpleDataframe1.show()\n",
        "simpleDataframe2 = spark.createDataFrame(data = simpleData2, schema = simpleschema2)\n",
        "simpleDataframe2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsjKnvxADVHV",
        "outputId": "072f1564-8fd3-48d3-f44d-eeabd3d41299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+\n",
            "| ID| NAME|DEPT|\n",
            "+---+-----+----+\n",
            "|  1|name1| cse|\n",
            "|  2|name2|extc|\n",
            "+---+-----+----+\n",
            "\n",
            "+---+-----+----+\n",
            "| ID| NAME|DEPT|\n",
            "+---+-----+----+\n",
            "|  1|name1| cse|\n",
            "|  2|name2|extc|\n",
            "+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# union we can use only if same number of attributies\n",
        "\n",
        "simpleDataunion = simpleDataframe1.union(simpleDataframe2)\n",
        "simpleDataunion.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlHCRTGYHG7T",
        "outputId": "3c617373-39bd-48b0-acb5-c3dcbbd43f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+\n",
            "| ID| NAME|DEPT|\n",
            "+---+-----+----+\n",
            "|  1|name1| cse|\n",
            "|  2|name2|extc|\n",
            "|  1|name1| cse|\n",
            "|  2|name2|extc|\n",
            "+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"join - Since the two DataFrames are identical in terms of both the\n",
        "schema and the data, the join will merge every row in the first\n",
        "DataFrame with every row in the second DataFrame, resulting\n",
        "in a Cartesian product \"\"\"\n",
        "simpleDatajoin = simpleDataframe1.join(simpleDataframe2)\n",
        "simpleDatajoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuDO35sVH5n7",
        "outputId": "6c67e064-e5d5-4691-8560-1fe4bf2c923c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+-----+----+\n",
            "| ID| NAME|DEPT| ID| NAME|DEPT|\n",
            "+---+-----+----+---+-----+----+\n",
            "|  1|name1| cse|  1|name1| cse|\n",
            "|  1|name1| cse|  2|name2|extc|\n",
            "|  2|name2|extc|  1|name1| cse|\n",
            "|  2|name2|extc|  2|name2|extc|\n",
            "+---+-----+----+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData11 = [(1,\"name1\",\"cse\",12),(2, \"name2\",\"extc\",21)]\n",
        "simpleschema11 = [\"ID\",\"NAME\",\"DEPT\", \"AGE\"]\n",
        "simpleDataframe11 = spark.createDataFrame(simpleData11, simpleschema11)\n",
        "simpleDataframe11.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNoxVHSRP7uZ",
        "outputId": "e55602dc-a01b-401c-db5b-f106870c1b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+\n",
            "|  1|name1| cse| 12|\n",
            "|  2|name2|extc| 21|\n",
            "+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#error as the number of columns not same\n",
        "simpleDataunion1 = simpleDataframe1.union(simpleDataframe11)\n",
        "simpleDataunion1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "0sB6uQjOQklw",
        "outputId": "79415164-5866-49e7-e80f-8536a0cdaba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- LogicalRDD [ID#157L, NAME#158, DEPT#159], false\n+- LogicalRDD [ID#195L, NAME#196, DEPT#197, AGE#198L], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c23004abb04f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimpleDataunion1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimpleDataframe1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleDataframe11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msimpleDataunion1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   3927\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3928\u001b[0m         \"\"\"\n\u001b[0;32m-> 3929\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- LogicalRDD [ID#157L, NAME#158, DEPT#159], false\n+- LogicalRDD [ID#195L, NAME#196, DEPT#197, AGE#198L], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join - as the columns differe, so no cross join instead inner join\n",
        "\n",
        "simpleDatajoin1 = simpleDataframe1.join(simpleDataframe11)\n",
        "simpleDatajoin1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESCE1o6qQ4Q4",
        "outputId": "ac05d301-5467-47cf-a306-5ee81fd1533b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+-----+----+---+\n",
            "| ID| NAME|DEPT| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+-----+----+---+\n",
            "|  1|name1| cse|  1|name1| cse| 12|\n",
            "|  1|name1| cse|  2|name2|extc| 21|\n",
            "|  2|name2|extc|  1|name1| cse| 12|\n",
            "|  2|name2|extc|  2|name2|extc| 21|\n",
            "+---+-----+----+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "simpleDataframe21 = simpleDataframe2.withColumn(\"AGE\", lit(\"null\"))\n",
        "simpleDataframe21.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9R0lL-PRW8f",
        "outputId": "e29231a6-321e-4dc7-e211-d32eb78daeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+----+\n",
            "| ID| NAME|DEPT| AGE|\n",
            "+---+-----+----+----+\n",
            "|  1|name1| cse|null|\n",
            "|  2|name2|extc|null|\n",
            "+---+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleDataunifies1 = simpleDataframe21.union(simpleDataframe11)\n",
        "simpleDataunifies1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTNWG4bSGjp",
        "outputId": "66822f44-e947-44f8-cd2a-8f9715b2e2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+----+\n",
            "| ID| NAME|DEPT| AGE|\n",
            "+---+-----+----+----+\n",
            "|  1|name1| cse|null|\n",
            "|  2|name2|extc|null|\n",
            "|  1|name1| cse|  12|\n",
            "|  2|name2|extc|  21|\n",
            "+---+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "same schema - two dataframes will do cross join\n",
        "\n",
        "\n",
        "same number of columns - union will work as long as datatype same\n"
      ],
      "metadata": {
        "id": "WlS49hV6CQQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData22 = [(1,\"name1\",\"cse\",11),(2, \"name2\",\"extc\",21)]\n",
        "simpleschema22 = [\"ID\",\"NAME\",\"DEPT\", \"AGE\"]\n",
        "simpleDataframe22 = spark.createDataFrame(simpleData22, simpleschema22)\n",
        "simpleDataframe22.show()\n",
        "simpleDataframe11.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKkw81yS3g3",
        "outputId": "68c86314-d6ed-4bd0-b4fe-e01131fe90c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+\n",
            "|  1|name1| cse| 11|\n",
            "|  2|name2|extc| 21|\n",
            "+---+-----+----+---+\n",
            "\n",
            "+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+\n",
            "|  1|name1| cse| 12|\n",
            "|  2|name2|extc| 21|\n",
            "+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross\n",
        "simpleDatacrossjoin = simpleDataframe11.join(simpleDataframe22)\n",
        "simpleDatacrossjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D18V4maW6nf",
        "outputId": "5f8c9490-a135-4350-ec48-ad283592a28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+---+-----+----+---+\n",
            "|  1|name1| cse| 12|  1|name1| cse| 11|\n",
            "|  1|name1| cse| 12|  2|name2|extc| 21|\n",
            "|  2|name2|extc| 21|  1|name1| cse| 11|\n",
            "|  2|name2|extc| 21|  2|name2|extc| 21|\n",
            "+---+-----+----+---+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inner\n",
        "simpleDatainnerjoin1 = simpleDataframe11.join(simpleDataframe22, on = \"ID\",how =\"inner\")\n",
        "simpleDatainnerjoin1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKvQT9O3XR_v",
        "outputId": "7c7296e7-fa30-4571-f049-0fbf9b107e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+-----+----+---+\n",
            "|  1|name1| cse| 12|name1| cse| 11|\n",
            "|  2|name2|extc| 21|name2|extc| 21|\n",
            "+---+-----+----+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#left-left outer\n",
        "simpleDataleftjoin = simpleDataframe11.join(simpleDataframe22, on = \"AGE\",how =\"left\")\n",
        "simpleDataleftjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhfub25VXxtH",
        "outputId": "7c86c6fa-88f4-4337-dc70-3800cb99302c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+----+----+-----+----+\n",
            "|AGE| ID| NAME|DEPT|  ID| NAME|DEPT|\n",
            "+---+---+-----+----+----+-----+----+\n",
            "| 12|  1|name1| cse|NULL| NULL|NULL|\n",
            "| 21|  2|name2|extc|   2|name2|extc|\n",
            "+---+---+-----+----+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#right-right outer\n",
        "simpleDatarightjoin = simpleDataframe11.join(simpleDataframe22, on = \"AGE\",how =\"right\")\n",
        "simpleDatarightjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpQr0pYzYa3a",
        "outputId": "5f550d74-d0ff-4cf6-8322-b441e24b6c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+----+---+-----+----+\n",
            "|AGE|  ID| NAME|DEPT| ID| NAME|DEPT|\n",
            "+---+----+-----+----+---+-----+----+\n",
            "| 11|NULL| NULL|NULL|  1|name1| cse|\n",
            "| 21|   2|name2|extc|  2|name2|extc|\n",
            "+---+----+-----+----+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#full outer\n",
        "simpleDatarightjoin = simpleDataframe11.join(simpleDataframe22, on = \"AGE\",how =\"outer\")\n",
        "simpleDatarightjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb7YGsCnY-As",
        "outputId": "6c447093-6aef-4951-ef01-fb39b2a29bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+----+----+-----+----+\n",
            "|AGE|  ID| NAME|DEPT|  ID| NAME|DEPT|\n",
            "+---+----+-----+----+----+-----+----+\n",
            "| 11|NULL| NULL|NULL|   1|name1| cse|\n",
            "| 12|   1|name1| cse|NULL| NULL|NULL|\n",
            "| 21|   2|name2|extc|   2|name2|extc|\n",
            "+---+----+-----+----+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#left-semi\n",
        "simpleDataleftsemijoin = simpleDataframe11.join(simpleDataframe22, on = \"AGE\",how =\"left_semi\")\n",
        "simpleDataleftsemijoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0uKkZEdZdmu",
        "outputId": "a774cdfd-bbef-4979-da27-a619d4e71ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+----+\n",
            "|AGE| ID| NAME|DEPT|\n",
            "+---+---+-----+----+\n",
            "| 21|  2|name2|extc|\n",
            "+---+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#left-anti\n",
        "simpleDataleftantijoin = simpleDataframe11.join(simpleDataframe22, on = \"AGE\",how =\"left_anti\")\n",
        "simpleDataleftantijoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ftB8l1GaVeT",
        "outputId": "cfc5fc99-38c4-49f4-ec13-c9fcadd49d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+----+\n",
            "|AGE| ID| NAME|DEPT|\n",
            "+---+---+-----+----+\n",
            "| 12|  1|name1| cse|\n",
            "+---+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining on multiple columns\n",
        "simpleDatamulcol = simpleDataframe11.join(simpleDataframe22, on = [\"ID\",\"NAME\"],how =\"inner\")\n",
        "simpleDatamulcol.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvo-CwExdCPW",
        "outputId": "21ba1af0-ac58-4095-ea94-de52bcfb3a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+----+---+\n",
            "| ID| NAME|DEPT|AGE|DEPT|AGE|\n",
            "+---+-----+----+---+----+---+\n",
            "|  1|name1| cse| 12| cse| 11|\n",
            "|  2|name2|extc| 21|extc| 21|\n",
            "+---+-----+----+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining on different column names\n",
        "simpleDatadiffcol = simpleDataframe11.join(simpleDataframe22,on = [simpleDataframe11.ID == simpleDataframe22.ID , simpleDataframe11.NAME == simpleDataframe22.NAME],how =\"inner\")\n",
        "simpleDatadiffcol.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZMpW8WGfsU2",
        "outputId": "b7200e51-a0b6-40c2-de45-78aa4a595039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+---+-----+----+---+\n",
            "| ID| NAME|DEPT|AGE| ID| NAME|DEPT|AGE|\n",
            "+---+-----+----+---+---+-----+----+---+\n",
            "|  1|name1| cse| 12|  1|name1| cse| 11|\n",
            "|  2|name2|extc| 21|  2|name2|extc| 21|\n",
            "+---+-----+----+---+---+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explode Column using Pyspark**"
      ],
      "metadata": {
        "id": "CTiqpuqHhPtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "data21 = [(1,[\"a\",\"b\"]),(2,[\"c\",\"l\"]),(3,[\"f\",\"u\"])]\n",
        "schema21 = [\"ID\",\"Name\"]\n",
        "dataframe21 = spark.createDataFrame(data21,schema21)\n",
        "dataframe21.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5cQIbjlhaad",
        "outputId": "258f8df6-a230-41a3-b41e-980f43c008ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| ID|  Name|\n",
            "+---+------+\n",
            "|  1|[a, b]|\n",
            "|  2|[c, l]|\n",
            "|  3|[f, u]|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe21explode = dataframe21.select(\"ID\",explode(\"Name\"))\n",
        "dataframe21explode.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksd1UKWnjm7E",
        "outputId": "2642f2bb-ab6f-4205-9996-b10d73a0070d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| ID|col|\n",
            "+---+---+\n",
            "|  1|  a|\n",
            "|  1|  b|\n",
            "|  2|  c|\n",
            "|  2|  l|\n",
            "|  3|  f|\n",
            "|  3|  u|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe22explode = dataframe21.withColumn(\"Name1\",explode(\"Name\"))\n",
        "dataframe22explode.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKuurdgZkq69",
        "outputId": "5e024149-c1ca-4673-a5cd-309edf3ff38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----+\n",
            "| ID|  Name|Name1|\n",
            "+---+------+-----+\n",
            "|  1|[a, b]|    a|\n",
            "|  1|[a, b]|    b|\n",
            "|  2|[c, l]|    c|\n",
            "|  2|[c, l]|    l|\n",
            "|  3|[f, u]|    f|\n",
            "|  3|[f, u]|    u|\n",
            "+---+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Solve using Regex**\n"
      ],
      "metadata": {
        "id": "3yI8mssFmUEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, rlike, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Regex\").getOrCreate()\n",
        "data30 = [(1,\"Shivam\",\"U67567879\"), (2,\"Sagar\",\"5678909876\"), (3, \"Muni\", \"45678u678\")]\n",
        "schema30 = [\"ID\",\"Name\",\"Mobileno\"]\n",
        "df30 = spark.createDataFrame(data30,schema30)\n",
        "df30.show()"
      ],
      "metadata": {
        "id": "YLOhH6Fkmcpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23db1ae0-3629-48df-d3fb-bfc988a36a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------+\n",
            "| ID|  Name|  Mobileno|\n",
            "+---+------+----------+\n",
            "|  1|Shivam| U67567879|\n",
            "|  2| Sagar|5678909876|\n",
            "|  3|  Muni| 45678u678|\n",
            "+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df30.filter(col(\"Mobileno\").rlike(\"^[0-9]+$\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOCv3A1FoeGL",
        "outputId": "47d8628e-e938-400e-cf27-3704875248ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+\n",
            "| ID| Name|  Mobileno|\n",
            "+---+-----+----------+\n",
            "|  2|Sagar|5678909876|\n",
            "+---+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***createOrReplaceTempView()***"
      ],
      "metadata": {
        "id": "abJeYcgxtb9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df30.createOrReplaceTempView(\"dataf30\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GxjqvuLvsHH5",
        "outputId": "1a921e18-c8e2-43e0-c115-6871c01c5214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df31 = spark.sql(\"\"\"select * from dataf30 where Mobileno REGEXP '^[0-9]+$'\"\"\")\n",
        "df31.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cfAEERqsq0H",
        "outputId": "3f85b59b-c17e-49ea-e9bd-2bf8c1f3ce46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+\n",
            "| ID| Name|  Mobileno|\n",
            "+---+-----+----------+\n",
            "|  2|Sagar|5678909876|\n",
            "+---+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Count rows in each column where NULLs present**"
      ],
      "metadata": {
        "id": "fiYzbPDiu1Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, when\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "data40 = [\n",
        "    (1, \"Alice\", \"1234567890\"),\n",
        "    (2, None,    \"9876543210\"),\n",
        "    (3, \"Charlie\", None),\n",
        "    (4, None,    None),\n",
        "    (None, \"Eve\", \"5556667777\")\n",
        "]\n",
        "\n",
        "# Schema definition\n",
        "schema40 = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Mobile\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "df40 = spark.createDataFrame(data40, schema40)\n",
        "\n",
        "df40.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz7Q6mQbublW",
        "outputId": "a49b87d7-fd0f-4ae1-c0c5-cd647a97c7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----------+\n",
            "|  ID|   Name|    Mobile|\n",
            "+----+-------+----------+\n",
            "|   1|  Alice|1234567890|\n",
            "|   2|   NULL|9876543210|\n",
            "|   3|Charlie|      NULL|\n",
            "|   4|   NULL|      NULL|\n",
            "|NULL|    Eve|5556667777|\n",
            "+----+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List comprehension python code bellow"
      ],
      "metadata": {
        "id": "ZNL7YuXiz8h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df41nulls = df40.select(\n",
        "    [sum(when(col(c).isNull(),1).otherwise(0)).alias(c+\"_Nulls\")\n",
        "    for c in df40.columns]\n",
        ")\n",
        "df41nulls.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJwhykPkvhva",
        "outputId": "f832d2e5-e08e-44a5-d7b6-f6a712ff23ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+\n",
            "|ID_Nulls|Name_Nulls|Mobile_Nulls|\n",
            "+--------+----------+------------+\n",
            "|       1|         2|           2|\n",
            "+--------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Handling multiple delimiters**"
      ],
      "metadata": {
        "id": "LCojtPCH0JQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, regexp_replace\n",
        "data5 = [\n",
        "    (1, \"apple,banana|cherry\"),\n",
        "    (2, \"grape;orange,kiwi\"),\n",
        "    (3, \"mango|lemon;pineapple\"),\n",
        "    (4, None)\n",
        "]\n",
        "\n",
        "columns5 = [\"ID\", \"Fruits\"]\n",
        "df5 = spark.createDataFrame(data5, columns5)\n",
        "df5.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwD45CuLzsUx",
        "outputId": "f80fe3b7-1d2d-4397-aaac-d9160aa6ffed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------+\n",
            "|ID |Fruits               |\n",
            "+---+---------------------+\n",
            "|1  |apple,banana|cherry  |\n",
            "|2  |grape;orange,kiwi    |\n",
            "|3  |mango|lemon;pineapple|\n",
            "|4  |NULL                 |\n",
            "+---+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df51 = df5.withColumn(\"Fruits_Normalized\",regexp_replace(\"Fruits\",\"[,|:;]\",\",\"))\n",
        "df51.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st_L-FKv1qro",
        "outputId": "92c24b32-2762-493f-e4ab-15df32a3f5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| ID|              Fruits|   Fruits_Normalized|\n",
            "+---+--------------------+--------------------+\n",
            "|  1| apple,banana|cherry| apple,banana,cherry|\n",
            "|  2|   grape;orange,kiwi|   grape,orange,kiwi|\n",
            "|  3|mango|lemon;pinea...|mango,lemon,pinea...|\n",
            "|  4|                NULL|                NULL|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df52 = df51.withColumn(\"Fruit_Split\", split(\"Fruits_Normalized\",\",\"))\n",
        "df52.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_dH1gXy3S1g",
        "outputId": "c9299d32-bdf6-4054-bcaa-dd76b748a728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+\n",
            "| ID|              Fruits|   Fruits_Normalized|         Fruit_Split|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "|  1| apple,banana|cherry| apple,banana,cherry|[apple, banana, c...|\n",
            "|  2|   grape;orange,kiwi|   grape,orange,kiwi|[grape, orange, k...|\n",
            "|  3|mango|lemon;pinea...|mango,lemon,pinea...|[mango, lemon, pi...|\n",
            "|  4|                NULL|                NULL|                NULL|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Using regexp_replace**"
      ],
      "metadata": {
        "id": "GER-mq4jZSAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, regexp_replace,col,trim\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Regexp\").getOrCreate()\n",
        "data6 = spark.createDataFrame([\" 1-12-23-2-34-45-3-45-45-4-56-67\"],\"String\")\n",
        "data6.show()"
      ],
      "metadata": {
        "id": "3QicP-jr5H4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6af0b4-f647-4168-8ea4-69d69a1e96e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "| 1-12-23-2-34-45-...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data61 = data6.withColumn(\"Value\", regexp_replace(col(\"value\"),\"(.*?\\\\-){3}\",\"$0,\"))\n",
        "data61.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN2-ea_iZnYE",
        "outputId": "edd7023f-10ed-47bc-d60d-6531aecf96c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|Value                              |\n",
            "+-----------------------------------+\n",
            "| 1-12-23-,2-34-45-,3-45-45-,4-56-67|\n",
            "+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data61.withColumn(\"Value\",explode(split(col(\"Value\"), \"-,\"))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BLID96McxCO",
        "outputId": "056e1fe9-0f2d-4cf1-f637-8f1098fa290f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|Value   |\n",
            "+--------+\n",
            "| 1-12-23|\n",
            "|2-34-45 |\n",
            "|3-45-45 |\n",
            "|4-56-67 |\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data62 = data61.withColumn(\"Value\",explode(split(trim(col(\"Value\")), \"-,\")))\n",
        "data62.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdl_JRFRfoRY",
        "outputId": "5e18fdcb-c18d-40c1-fce2-20decffd8909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Value  |\n",
            "+-------+\n",
            "|1-12-23|\n",
            "|2-34-45|\n",
            "|3-45-45|\n",
            "|4-56-67|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data63 = data62.withColumn(\"Value\",split(col(\"Value\"),\"-\"))\n",
        "data63.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6r7mDZYg318",
        "outputId": "d2d35402-3897-4b3f-9711-86473250f311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|      Value|\n",
            "+-----------+\n",
            "|[1, 12, 23]|\n",
            "|[2, 34, 45]|\n",
            "|[3, 45, 45]|\n",
            "|[4, 56, 67]|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data64 = data63.withColumn(\"Age\",col(\"Value\")[0]).withColumn(\"ID\",col(\"Value\")[1]).withColumn(\"No\", col(\"Value\")[2])\n",
        "data64.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55e7nlfWhUYa",
        "outputId": "177a98fc-0bde-47c8-a2eb-f88a39fef573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---+---+---+\n",
            "|      Value|Age| ID| No|\n",
            "+-----------+---+---+---+\n",
            "|[1, 12, 23]|  1| 12| 23|\n",
            "|[2, 34, 45]|  2| 34| 45|\n",
            "|[3, 45, 45]|  3| 45| 45|\n",
            "|[4, 56, 67]|  4| 56| 67|\n",
            "+-----------+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Pivoting data - Name- rows to new columns **"
      ],
      "metadata": {
        "id": "5f3WppzgmI1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data70 = [(1,'Gaga','India',\"2022-01-11\"),(1,'Katy','UK', \"2022-01-11\"),(1,'Bey','Europe', \"2022-01-11\"),(2,'Gaga',None, \"2022-10-11\"),(2,'Katy','India', \"2022-10-11\"),(2,'Bey','US',\"2022-02-15\"),(3,'Gaga','Europe', \"2022-10-11\"),\n",
        "(3,'Katy','US',\"2022-10-11\"),(3,'Bey',None,\"2022-02-15\"), (1, 'Gaga','US',\"2022-01-11\"),(3, 'Katy', 'Switz',\"2022-02-15\") ]\n",
        "dataf70 = spark.createDataFrame(data70,['ID','NAME','COUNTRY', 'Date_part'])\n",
        "dataf70.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Faa5L_kKhlhw",
        "outputId": "40c33516-d0c4-4781-e6af-1e2bb94590f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+----------+\n",
            "| ID|NAME|COUNTRY| Date_part|\n",
            "+---+----+-------+----------+\n",
            "|  1|Gaga|  India|2022-01-11|\n",
            "|  1|Katy|     UK|2022-01-11|\n",
            "|  1| Bey| Europe|2022-01-11|\n",
            "|  2|Gaga|   NULL|2022-10-11|\n",
            "|  2|Katy|  India|2022-10-11|\n",
            "|  2| Bey|     US|2022-02-15|\n",
            "|  3|Gaga| Europe|2022-10-11|\n",
            "|  3|Katy|     US|2022-10-11|\n",
            "|  3| Bey|   NULL|2022-02-15|\n",
            "|  1|Gaga|     US|2022-01-11|\n",
            "|  3|Katy|  Switz|2022-02-15|\n",
            "+---+----+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we want other  entries of countries  also in the name columns hence collect_list"
      ],
      "metadata": {
        "id": "iqtkLmSSwXao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_list, first\n",
        "data71 = dataf70.groupBy(\"ID\",\"Date_part\").pivot(\"NAME\").agg(collect_list(\"COUNTRY\"))\n",
        "data71.show()\n",
        "display(data71)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "YsaXypCNopWT",
        "outputId": "a3005b3d-fbdc-4680-a815-eb2c5c6ad222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------+-----------+-------+\n",
            "| ID| Date_part|     Bey|       Gaga|   Katy|\n",
            "+---+----------+--------+-----------+-------+\n",
            "|  1|2022-01-11|[Europe]|[India, US]|   [UK]|\n",
            "|  2|2022-10-11|      []|         []|[India]|\n",
            "|  2|2022-02-15|    [US]|         []|     []|\n",
            "|  3|2022-02-15|      []|         []|[Switz]|\n",
            "|  3|2022-10-11|      []|   [Europe]|   [US]|\n",
            "+---+----------+--------+-----------+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[ID: bigint, Date_part: string, Bey: array<string>, Gaga: array<string>, Katy: array<string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import arrays_zip\n",
        "data72 = data71.withColumn(\"new\", arrays_zip(\"Bey\",\"Gaga\",\"Katy\")).withColumn(\"new1\",explode(\"new\")).drop(\"new\")\n",
        "data72.show(truncate = False)\n",
        "data73 = data72.select(\"ID\",\"Date_part\",\"new1.Bey\",\"new1.Gaga\",\"new1.Katy\")\n",
        "display(data72)\n",
        "data73.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "dcAJ_slywwsh",
        "outputId": "3de1ea47-fc7e-407d-ceec-7b3d387019f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "|ID |Date_part |Bey     |Gaga       |Katy   |new1               |\n",
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "|1  |2022-01-11|[Europe]|[India, US]|[UK]   |{Europe, India, UK}|\n",
            "|1  |2022-01-11|[Europe]|[India, US]|[UK]   |{NULL, US, NULL}   |\n",
            "|2  |2022-10-11|[]      |[]         |[India]|{NULL, NULL, India}|\n",
            "|2  |2022-02-15|[US]    |[]         |[]     |{US, NULL, NULL}   |\n",
            "|3  |2022-02-15|[]      |[]         |[Switz]|{NULL, NULL, Switz}|\n",
            "|3  |2022-10-11|[]      |[Europe]   |[US]   |{NULL, Europe, US} |\n",
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[ID: bigint, Date_part: string, Bey: array<string>, Gaga: array<string>, Katy: array<string>, new1: struct<Bey:string,Gaga:string,Katy:string>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+------+-----+\n",
            "| ID| Date_part|   Bey|  Gaga| Katy|\n",
            "+---+----------+------+------+-----+\n",
            "|  1|2022-01-11|Europe| India|   UK|\n",
            "|  1|2022-01-11|  NULL|    US| NULL|\n",
            "|  2|2022-10-11|  NULL|  NULL|India|\n",
            "|  2|2022-02-15|    US|  NULL| NULL|\n",
            "|  3|2022-02-15|  NULL|  NULL|Switz|\n",
            "|  3|2022-10-11|  NULL|Europe|   US|\n",
            "+---+----------+------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_cols = data71pivot.columns[2:]\n",
        "for i in range(len(list_of_cols)):\n",
        "  list_of_cols[i] = \"new1.\"+list_of_cols[i]\n",
        "data72 = data71.withColumn(\"new\", arrays_zip(*data71pivot.columns[2:])).withColumn(\"new1\",explode(\"new\")).drop(\"new\")\n",
        "data72.show(truncate = False)\n",
        "data73 = data72.select(*data71pivot.columns[:2],*list_of_cols)\n",
        "display(data72)\n",
        "data73.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "KxvrtcfH0-3i",
        "outputId": "6c04f48d-7af7-4ad9-a8eb-9e8225a8d74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "|ID |Date_part |Bey     |Gaga       |Katy   |new1               |\n",
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "|1  |2022-01-11|[Europe]|[India, US]|[UK]   |{Europe, India, UK}|\n",
            "|1  |2022-01-11|[Europe]|[India, US]|[UK]   |{NULL, US, NULL}   |\n",
            "|2  |2022-10-11|[]      |[]         |[India]|{NULL, NULL, India}|\n",
            "|2  |2022-02-15|[US]    |[]         |[]     |{US, NULL, NULL}   |\n",
            "|3  |2022-02-15|[]      |[]         |[Switz]|{NULL, NULL, Switz}|\n",
            "|3  |2022-10-11|[]      |[Europe]   |[US]   |{NULL, Europe, US} |\n",
            "+---+----------+--------+-----------+-------+-------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[ID: bigint, Date_part: string, Bey: array<string>, Gaga: array<string>, Katy: array<string>, new1: struct<Bey:string,Gaga:string,Katy:string>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+------+-----+\n",
            "| ID| Date_part|   Bey|  Gaga| Katy|\n",
            "+---+----------+------+------+-----+\n",
            "|  1|2022-01-11|Europe| India|   UK|\n",
            "|  1|2022-01-11|  NULL|    US| NULL|\n",
            "|  2|2022-10-11|  NULL|  NULL|India|\n",
            "|  2|2022-02-15|    US|  NULL| NULL|\n",
            "|  3|2022-02-15|  NULL|  NULL|Switz|\n",
            "|  3|2022-10-11|  NULL|Europe|   US|\n",
            "+---+----------+------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Check the value of Nulls in each Column**"
      ],
      "metadata": {
        "id": "nAk_g6Q8xVd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, col, count, when\n",
        "spark = SparkSession.builder.appName(\"CountNulls\").getOrCreate()\n",
        "data8=[(1,'Sagar',23),(2,None,34),(None,None,40),(5,'Alex',None),(4,'Kim',None)]\n",
        "data80=spark.createDataFrame(data8,schema=\"ID int,Name string,Age int\")\n",
        "data80.show()"
      ],
      "metadata": {
        "id": "8FBzdohv8BWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58497dc7-8965-4286-952a-9533f5768c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|  ID| Name| Age|\n",
            "+----+-----+----+\n",
            "|   1|Sagar|  23|\n",
            "|   2| NULL|  34|\n",
            "|NULL| NULL|  40|\n",
            "|   5| Alex|NULL|\n",
            "|   4|  Kim|NULL|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data81 = data80.select( [ count(when(col(i).isNull(),i)).alias(i+\"_Null\") for i in data80.columns ])\n",
        "data81.show()"
      ],
      "metadata": {
        "id": "zJwMsXNL8BFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1f20a6-5ff9-4f60-cafb-aab7fd463455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+--------+\n",
            "|ID_Null|Name_Null|Age_Null|\n",
            "+-------+---------+--------+\n",
            "|      1|        2|       2|\n",
            "+-------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Solve using regexp_extract method**"
      ],
      "metadata": {
        "id": "-l4ogfSS2UL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data9=[('ABSHFJFJ12QWERT12',1),('QWERT5674OTUT1',2),('DGDGNJDJ1234UYI',3)]\n",
        "data90=spark.createDataFrame(data9,schema=\"input_string string,id int\")\n",
        "data90.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R40j-XAZzlM8",
        "outputId": "73db0232-cbc6-4df7-9042-9f0b3c9b4b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|     input_string| id|\n",
            "+-----------------+---+\n",
            "|ABSHFJFJ12QWERT12|  1|\n",
            "|   QWERT5674OTUT1|  2|\n",
            "|  DGDGNJDJ1234UYI|  3|\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "data91 = data90.withColumn(\"new_col\", regexp_extract(data90.input_string, \"(^[a-zA-Z]*[0-9]*)\" , 1))\n",
        "data91.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_eemACR2qKR",
        "outputId": "ffc7cc81-e931-4ef1-ce04-24a5df9002b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+------------+\n",
            "|     input_string| id|     new_col|\n",
            "+-----------------+---+------------+\n",
            "|ABSHFJFJ12QWERT12|  1|  ABSHFJFJ12|\n",
            "|   QWERT5674OTUT1|  2|   QWERT5674|\n",
            "|  DGDGNJDJ1234UYI|  3|DGDGNJDJ1234|\n",
            "+-----------------+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data92 = data91.withColumn(\"new_col2\", regexp_extract(data91.input_string,\"([a-zA-Z]*[0-9]*$)\",1))\n",
        "data92.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHvZuLLG4hve",
        "outputId": "a6834b7e-b156-4ed8-8fc9-f134a062025c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+------------+--------+\n",
            "|     input_string| id|     new_col|new_col2|\n",
            "+-----------------+---+------------+--------+\n",
            "|ABSHFJFJ12QWERT12|  1|  ABSHFJFJ12| QWERT12|\n",
            "|   QWERT5674OTUT1|  2|   QWERT5674|   OTUT1|\n",
            "|  DGDGNJDJ1234UYI|  3|DGDGNJDJ1234|     UYI|\n",
            "+-----------------+---+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10."
      ],
      "metadata": {
        "id": "YQiLwmTMQk7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='/FileStore/tables/csv_with_no_header.csv'\n",
        "df=spark.read.option('header',True).option('inferschema',True).csv(f'{path}').limit(1)\n",
        "if(df.schema==schema):\n",
        "    df.write.mode('overwrite').save('/FileStore/tables/csv_with_header_output/')\n",
        "else:\n",
        "    df=spark.read.option('header',False).schema(schema).csv(f'{path}')\n",
        "    df.write.mode('overwrite').save('/FileStore/tables/csv_with_header_output/')"
      ],
      "metadata": {
        "id": "6hsK4xUt6Wyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Solve using REGEXP_REPLACE and REGEXP_EXTRACT in PySpark**"
      ],
      "metadata": {
        "id": "mCQK-sYSadHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col, explode\n",
        "spark = SparkSession.builder.appName(\"regexp\").getOrCreate()\n",
        "data110=[(1,\"Sagar-Prajapati\"),(2,\"Alex-John\"),(3,\"John Cena\"),(4,\"Kim Joe\")]\n",
        "schema110=\"ID int,Name string\"\n",
        "data11=spark.createDataFrame(data110,schema110)\n",
        "data11.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVLV2qX6acs4",
        "outputId": "5c3879f6-5ee4-4cb1-f8f2-1980aa9b035d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+\n",
            "| ID|           Name|\n",
            "+---+---------------+\n",
            "|  1|Sagar-Prajapati|\n",
            "|  2|      Alex-John|\n",
            "|  3|      John Cena|\n",
            "|  4|        Kim Joe|\n",
            "+---+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data111 = data11.select(split(col(\"Name\"),\"[- ]\").alias(\"Names\")).withColumn(\"First_name\",col(\"Names\")[0]).withColumn(\"Last_name\", col(\"Names\")[1])\n",
        "data111.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDbJodKkbJFn",
        "outputId": "3dadee50-cb53-423f-8ed9-a0b0dc687de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----------+---------+\n",
            "|             Names|First_name|Last_name|\n",
            "+------------------+----------+---------+\n",
            "|[Sagar, Prajapati]|     Sagar|Prajapati|\n",
            "|      [Alex, John]|      Alex|     John|\n",
            "|      [John, Cena]|      John|     Cena|\n",
            "|        [Kim, Joe]|       Kim|      Joe|\n",
            "+------------------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not \"boring\".Return the result table in descending order by rating.**"
      ],
      "metadata": {
        "id": "J_LoWwsphZ8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data120=[(1, 'War','great 3D',8.9)\n",
        ",(2, 'Science','fiction',8.5)\n",
        ",(3, 'irish','boring',6.2)\n",
        ",(4, 'Ice song','Fantacy',8.6)\n",
        ",(5, 'House card','Interesting',9.1)]\n",
        "schema120=\"ID int,movie string,description string,rating double\"\n",
        "data12=spark.createDataFrame(data120,schema120)\n",
        "data12.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWd6PAaMcGcd",
        "outputId": "32bdd5a8-759f-4329-a839-d8023f506341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+------+\n",
            "| ID|     movie|description|rating|\n",
            "+---+----------+-----------+------+\n",
            "|  1|       War|   great 3D|   8.9|\n",
            "|  2|   Science|    fiction|   8.5|\n",
            "|  3|     irish|     boring|   6.2|\n",
            "|  4|  Ice song|    Fantacy|   8.6|\n",
            "|  5|House card|Interesting|   9.1|\n",
            "+---+----------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data121 = data12.select(\"*\").where((col(\"ID\")%2==1) & (col(\"description\")!=\"boring\") )\n",
        "data121.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-LoYhjZh_M3",
        "outputId": "c68a93c9-8da2-4367-8ab7-2d0cdeb77183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+------+\n",
            "| ID|     movie|description|rating|\n",
            "+---+----------+-----------+------+\n",
            "|  1|       War|   great 3D|   8.9|\n",
            "|  5|House card|Interesting|   9.1|\n",
            "+---+----------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data12.createOrReplaceTempView(\"view12\")\n",
        "data122 = spark.sql(\"\"\"select * from view12 where ID%2 == 1 and description != 'boring'\"\"\")\n",
        "data122.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojRWx3mViVRI",
        "outputId": "5907c26b-5add-40b9-fc46-f6fdcc439cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+------+\n",
            "| ID|     movie|description|rating|\n",
            "+---+----------+-----------+------+\n",
            "|  1|       War|   great 3D|   8.9|\n",
            "|  5|House card|Interesting|   9.1|\n",
            "+---+----------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Solve using PySpark- Collect_list and Aggregation**"
      ],
      "metadata": {
        "id": "-mKv6Y4AgzxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data130 = [\n",
        "    (\"john\", \"tomato\", 2),\n",
        "    (\"𝚋𝚒𝚕𝚕\", \"𝚊𝚙𝚙𝚕𝚎\", 2),\n",
        "    (\"john\", \"𝚋𝚊𝚗𝚊𝚗𝚊\", 2),\n",
        "    (\"john\", \"tomato\", 3),\n",
        "    (\"𝚋𝚒𝚕𝚕\", \"𝚝𝚊𝚌𝚘\", 2),\n",
        "    (\"𝚋𝚒𝚕𝚕\", \"𝚊𝚙𝚙𝚕𝚎\", 2)\n",
        "]\n",
        "schema130 = \"name string,item string,weight int\"\n",
        "data13 = spark.createDataFrame(data130, schema130)\n",
        "data13.show()"
      ],
      "metadata": {
        "id": "xkwsVG-XjeNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc5799a-eabd-4e94-8835-ea221ce94ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+------+\n",
            "|    name|        item|weight|\n",
            "+--------+------------+------+\n",
            "|    john|      tomato|     2|\n",
            "|𝚋𝚒𝚕𝚕|  𝚊𝚙𝚙𝚕𝚎|     2|\n",
            "|    john|𝚋𝚊𝚗𝚊𝚗𝚊|     2|\n",
            "|    john|      tomato|     3|\n",
            "|𝚋𝚒𝚕𝚕|    𝚝𝚊𝚌𝚘|     2|\n",
            "|𝚋𝚒𝚕𝚕|  𝚊𝚙𝚙𝚕𝚎|     2|\n",
            "+--------+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col, explode, sum, struct\n",
        "spark = SparkSession.builder.appName(\"Collect_List\").getOrCreate()\n",
        "from pyspark.sql.functions import collect_list\n",
        "data131 = data13.groupBy(\"name\",\"item\").agg(sum(\"weight\").alias(\"Sum\"))\n",
        "data131.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXK1N_YlluEz",
        "outputId": "180d92cf-3924-43cb-c1ff-4edd7a4b4c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+---+\n",
            "|name    |item        |Sum|\n",
            "+--------+------------+---+\n",
            "|𝚋𝚒𝚕𝚕|𝚊𝚙𝚙𝚕𝚎  |4  |\n",
            "|john    |tomato      |5  |\n",
            "|john    |𝚋𝚊𝚗𝚊𝚗𝚊|2  |\n",
            "|𝚋𝚒𝚕𝚕|𝚝𝚊𝚌𝚘    |2  |\n",
            "+--------+------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data132 = data131.groupBy(\"name\").agg(collect_list(struct(\"item\",\"sum\")).alias(\"List\"))\n",
        "data132.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVGdBR0gmZoO",
        "outputId": "0e4a794c-63eb-4b30-f653-d148c851e5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------+\n",
            "|name    |List                            |\n",
            "+--------+--------------------------------+\n",
            "|𝚋𝚒𝚕𝚕|[{𝚊𝚙𝚙𝚕𝚎, 4}, {𝚝𝚊𝚌𝚘, 2}]|\n",
            "|john    |[{tomato, 5}, {𝚋𝚊𝚗𝚊𝚗𝚊, 2}]|\n",
            "+--------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.  Write a pyspark dataframe query to find all duplicate emails | IBM Interview Question |**"
      ],
      "metadata": {
        "id": "P6ajzeScg00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data140 = [(1, \"abc@gmail.com\"), (2, \"bcd@gmail.com\"), (3, \"abc@gmail.com\")]\n",
        "schema140 = \"ID int,email string\"\n",
        "data141 = spark.createDataFrame(data140, schema140)\n",
        "data141.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEG2iK6lgSMn",
        "outputId": "2977b8a3-8f66-4931-9506-981b151621ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+\n",
            "| ID|        email|\n",
            "+---+-------------+\n",
            "|  1|abc@gmail.com|\n",
            "|  2|bcd@gmail.com|\n",
            "|  3|abc@gmail.com|\n",
            "+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "data142 = data141.groupBy(\"email\").agg(count(\"email\").alias(\"dups\")).where( col(\"dups\") > 1)\n",
        "data142.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IoK64jThM-7",
        "outputId": "c7ffb170-a67d-48ab-ce99-472b9a1cb84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|        email|dups|\n",
            "+-------------+----+\n",
            "|abc@gmail.com|   2|\n",
            "+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 . Find Customers who never ordered anything."
      ],
      "metadata": {
        "id": "JQGFuxJWmG3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data150=[(1,'Sagar'),(2,'Alex'),(3,'John'),(4,'Kim')]\n",
        "schema150=\"Customer_ID int, Customer_Name string\"\n",
        "data15=spark.createDataFrame(data150,schema150)\n",
        "data15.show()\n",
        "\n",
        "data151=[(1,4),(3,2)]\n",
        "schema151=\"Order_ID int, Customer_ID int\"\n",
        "data151=spark.createDataFrame(data151,schema151)\n",
        "data151.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxd7hvIcif5-",
        "outputId": "7e92d7b1-afde-460f-e161-213576d410e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|Customer_ID|Customer_Name|\n",
            "+-----------+-------------+\n",
            "|          1|        Sagar|\n",
            "|          2|         Alex|\n",
            "|          3|         John|\n",
            "|          4|          Kim|\n",
            "+-----------+-------------+\n",
            "\n",
            "+--------+-----------+\n",
            "|Order_ID|Customer_ID|\n",
            "+--------+-----------+\n",
            "|       1|          4|\n",
            "|       3|          2|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data152 = data15.join(data151,\"Customer_ID\",\"left_anti\")\n",
        "data152.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jre7h_gHnGM-",
        "outputId": "30e885cd-6be8-4fd7-e458-0a7a14f3a2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|Customer_ID|Customer_Name|\n",
            "+-----------+-------------+\n",
            "|          1|        Sagar|\n",
            "|          3|         John|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. Write a report that provides customer id from customer table that brought all the products in product table.**"
      ],
      "metadata": {
        "id": "xZ82hbympYiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data160=[(1,5),(2,6),(3,5),(3,6),(1,6)]\n",
        "schema160=\"customer_id int,product_key int\"\n",
        "data16=spark.createDataFrame(data160,schema160)\n",
        "data16.show()\n",
        "\n",
        "data161=[(5,),(6,)]\n",
        "schema161=\"product_key int\"\n",
        "data162=spark.createDataFrame(data161,schema161)\n",
        "data162.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk3HggJjo_No",
        "outputId": "c6ba3e90-8550-4c99-9642-6e67f51befdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|customer_id|product_key|\n",
            "+-----------+-----------+\n",
            "|          1|          5|\n",
            "|          2|          6|\n",
            "|          3|          5|\n",
            "|          3|          6|\n",
            "|          1|          6|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+\n",
            "|product_key|\n",
            "+-----------+\n",
            "|          5|\n",
            "|          6|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count_distinct\n",
        "data163 = data16.groupBy(\"customer_id\").agg(count_distinct(\"product_key\").alias(\"distinct_product_key\"))\n",
        "data163.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-BANK2SZNOt",
        "outputId": "61ea7300-8a51-43a0-9bab-fae5685879e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+\n",
            "|customer_id|distinct_product_key|\n",
            "+-----------+--------------------+\n",
            "|          1|                   2|\n",
            "|          3|                   2|\n",
            "|          2|                   1|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data164 = data162.select(count_distinct(\"product_key\").alias(\"distinct_product_key\"))\n",
        "data164.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCi0dz0rbbRR",
        "outputId": "b21bed45-464b-404d-a571-ea5c71987ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|distinct_product_key|\n",
            "+--------------------+\n",
            "|                   2|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data163.join(data164,\"distinct_product_key\",\"inner\").select(\"customer_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VhPazykb0QL",
        "outputId": "a6da963a-6948-4848-a0da-338d7dd7a7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|          1|\n",
            "|          3|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. Get employees,dept id with max and min salary in each department**"
      ],
      "metadata": {
        "id": "fuWa5Au0ekDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import min, max, collect_list, struct, col\n",
        "spark = SparkSession.builder.appName(\"min-max\").getOrCreate()\n",
        "data170=[('Genece' , 2 , 75000),\n",
        "('𝗝𝗮𝗶𝗺𝗶𝗻' , 2 , 80000),\n",
        "('𝗣𝗮𝗻𝗸𝗮𝗷' , 2 , 80000),\n",
        "('Tarvares' , 2 , 70000),\n",
        "('Marlania' , 4 , 70000),\n",
        "('Briana' , 4 , 85000),\n",
        "('𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶' , 4 , 55000),\n",
        "('𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮' , 4 , 55000),\n",
        "('Lakken', 5, 60000),\n",
        "('Latoynia' , 5 , 65000) ]\n",
        "schema170=\"emp_name string,dept_id int,salary int\"\n",
        "data17=spark.createDataFrame(data170,schema170)\n",
        "data17.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sfjfVP_eUyW",
        "outputId": "d9458f66-04e1-471c-c0a0-a3b70fbea717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+------+\n",
            "|          emp_name|dept_id|salary|\n",
            "+------------------+-------+------+\n",
            "|            Genece|      2| 75000|\n",
            "|      𝗝𝗮𝗶𝗺𝗶𝗻|      2| 80000|\n",
            "|      𝗣𝗮𝗻𝗸𝗮𝗷|      2| 80000|\n",
            "|          Tarvares|      2| 70000|\n",
            "|          Marlania|      4| 70000|\n",
            "|            Briana|      4| 85000|\n",
            "|  𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶|      4| 55000|\n",
            "|𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮|      4| 55000|\n",
            "|            Lakken|      5| 60000|\n",
            "|          Latoynia|      5| 65000|\n",
            "+------------------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cannot use any agg inside struct...."
      ],
      "metadata": {
        "id": "P_7OIigzMKOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data171 = data17.groupBy(\"dept_id\").agg(min(\"salary\").alias(\"Min\") , max(\"salary\").alias(\"Max\"))\n",
        "data171 .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQaqeN3EfBw_",
        "outputId": "b20472a6-a419-4c73-db73-6594218cf448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|dept_id|  Min|  Max|\n",
            "+-------+-----+-----+\n",
            "|      4|55000|85000|\n",
            "|      2|70000|80000|\n",
            "|      5|60000|65000|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data172 = data17.join(data171, (data17.dept_id == data171.dept_id) & (data17.salary == data171.Min), \"right\").select(col(\"emp_name\").alias(\"emp_name_min\"), data17.dept_id, \"Min\" )\n",
        "data172.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q91JXOgofydO",
        "outputId": "050cd3cf-bc30-4a8c-fb26-44c8dc134eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+-----+\n",
            "|      emp_name_min|dept_id|  Min|\n",
            "+------------------+-------+-----+\n",
            "|𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮|      4|55000|\n",
            "|  𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶|      4|55000|\n",
            "|          Tarvares|      2|70000|\n",
            "|            Lakken|      5|60000|\n",
            "+------------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data173 = data17.join(data171, (data17.dept_id == data171.dept_id) & (data17.salary == data171.Max), \"right\").select(col(\"emp_name\").alias(\"emp_name_max\"), data17.dept_id, \"Max\" )\n",
        "data173.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qncKuJHmNvAO",
        "outputId": "24e4ba87-a7c1-4d41-c6d0-41cd2534842b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+-----+\n",
            "|emp_name_max|dept_id|  Max|\n",
            "+------------+-------+-----+\n",
            "|      Briana|      4|85000|\n",
            "|𝗣𝗮𝗻𝗸𝗮𝗷|      2|80000|\n",
            "|𝗝𝗮𝗶𝗺𝗶𝗻|      2|80000|\n",
            "|    Latoynia|      5|65000|\n",
            "+------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data174 = data172.union(data173)\n",
        "data174.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsuHnOGdPkYP",
        "outputId": "0308b79b-befb-4baa-a721-63b3b67edfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+-----+\n",
            "|      emp_name_min|dept_id|  Min|\n",
            "+------------------+-------+-----+\n",
            "|𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮|      4|55000|\n",
            "|  𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶|      4|55000|\n",
            "|          Tarvares|      2|70000|\n",
            "|            Lakken|      5|60000|\n",
            "|            Briana|      4|85000|\n",
            "|      𝗣𝗮𝗻𝗸𝗮𝗷|      2|80000|\n",
            "|      𝗝𝗮𝗶𝗺𝗶𝗻|      2|80000|\n",
            "|          Latoynia|      5|65000|\n",
            "+------------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data174.groupBy(\"dept_id\",\"Min\").agg(collect_list(\"emp_name_min\")).orderBy(\"dept_id\",\"Min\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri-sAb9MPrT8",
        "outputId": "ad420839-c5f3-4e9a-8318-6756247c06b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------------------------------------+\n",
            "|dept_id|Min  |collect_list(emp_name_min)            |\n",
            "+-------+-----+--------------------------------------+\n",
            "|2      |70000|[Tarvares]                            |\n",
            "|2      |80000|[𝗣𝗮𝗻𝗸𝗮𝗷, 𝗝𝗮𝗶𝗺𝗶𝗻]          |\n",
            "|4      |55000|[𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮, 𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶]|\n",
            "|4      |85000|[Briana]                              |\n",
            "|5      |60000|[Lakken]                              |\n",
            "|5      |65000|[Latoynia]                            |\n",
            "+-------+-----+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. Cache and Persist - % of nulls in each column**"
      ],
      "metadata": {
        "id": "for1K5uSXdkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data180 = [(\"Raj\",\"Doe\",None),\n",
        " (None,\"Samuel\",\"VIZAG\"),\n",
        " (\"David\",\"Smith\", None),\n",
        " (\"Samson\",None, \"HYD\"),\n",
        " (\"Immi\", \"Steve\", \"BNG\"),\n",
        " (None, None, None)]\n",
        "\n",
        "columns180 = [\"Firstname\", \"Lastname\", \"City\"]\n",
        "\n",
        "data18 = spark.createDataFrame(data180,columns180)\n",
        "data18.cache()\n",
        "data18.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAvTJBg8Rjnw",
        "outputId": "55a4286f-f24f-4f4a-e849-9cbb552724a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-----+\n",
            "|Firstname|Lastname| City|\n",
            "+---------+--------+-----+\n",
            "|      Raj|     Doe| NULL|\n",
            "|     NULL|  Samuel|VIZAG|\n",
            "|    David|   Smith| NULL|\n",
            "|   Samson|    NULL|  HYD|\n",
            "|     Immi|   Steve|  BNG|\n",
            "|     NULL|    NULL| NULL|\n",
            "+---------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**when summing, include otherwise also to get null counts else use count only.**"
      ],
      "metadata": {
        "id": "l2iV62gNb2O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when, round\n",
        "\n",
        "data18.select([ count(when(col(i).isNull(), 1)).alias(i+\"_Null\") for i in data18.columns ] ).show()\n",
        "data18.select([ sum(when(col(i).isNull(), 1).otherwise(0)).alias(i+\"_Null\") for i in data18.columns ] ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPHIBHhDX0yQ",
        "outputId": "26325b04-2124-4cc1-b9c8-791d43568b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+---------+\n",
            "|Firstname_Null|Lastname_Null|City_Null|\n",
            "+--------------+-------------+---------+\n",
            "|             2|            2|        3|\n",
            "+--------------+-------------+---------+\n",
            "\n",
            "+--------------+-------------+---------+\n",
            "|Firstname_Null|Lastname_Null|City_Null|\n",
            "+--------------+-------------+---------+\n",
            "|             2|            2|        3|\n",
            "+--------------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = data18.count()\n",
        "data18.select([ round((count(when(col(i).isNull(), 1))/total*100.0),2).alias(i+\"_Null_%\") for i in data18.columns ] ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nztwVKXYvnG",
        "outputId": "5e80cd11-28c8-4c81-cb60-d2b1747439d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------------+-----------+\n",
            "|Firstname_Null_%|Lastname_Null_%|City_Null_%|\n",
            "+----------------+---------------+-----------+\n",
            "|           33.33|          33.33|       50.0|\n",
            "+----------------+---------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "for i in data18.columns:\n",
        "  null_count = data18.filter(col(i).isNull()).count()\n",
        "  total = data18.select(col(i)).count()\n",
        "  percentage = null_count/total*100\n",
        "  print(percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ9V6edCca1m",
        "outputId": "a54cb201-079b-4cd5-838c-5caf53a4213b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33.33333333333333\n",
            "33.33333333333333\n",
            "50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Coalese and explode"
      ],
      "metadata": {
        "id": "14fLcOgdi7bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data190 = [('Alice', ['Badminton','Tennis'] ), ('Bob',['Tennis','Cricket']),('Julie',['Cricket','Carrom'])]\n",
        "schema190 = \"Name string,Hobbies array<string>\"\n",
        "data19 = spark.createDataFrame(data190,schema190)\n",
        "data19.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dnO2eAgi6rC",
        "outputId": "6b30ae78-9b19-465a-f30b-3c294bf6b8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+\n",
            "| Name|            Hobbies|\n",
            "+-----+-------------------+\n",
            "|Alice|[Badminton, Tennis]|\n",
            "|  Bob|  [Tennis, Cricket]|\n",
            "|Julie|  [Cricket, Carrom]|\n",
            "+-----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "data19.select( \"Name\",explode('Hobbies')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWO-pt0zkmAs",
        "outputId": "c4f878da-470e-434e-a974-4060f9528378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "| Name|      col|\n",
            "+-----+---------+\n",
            "|Alice|Badminton|\n",
            "|Alice|   Tennis|\n",
            "|  Bob|   Tennis|\n",
            "|  Bob|  Cricket|\n",
            "|Julie|  Cricket|\n",
            "|Julie|   Carrom|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data191 = [('Goa','','AP'),('','AP',None),(None,'','Blore')]\n",
        "schema191 = 'City1 string,City2 string,City3 string'\n",
        "data192 = spark.createDataFrame(data191, schema191)\n",
        "data192.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHR5G-hZfaPO",
        "outputId": "9207e2b7-3f22-42a0-dab5-92c4975d8a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+\n",
            "|City1|City2|City3|\n",
            "+-----+-----+-----+\n",
            "|  Goa|     |   AP|\n",
            "|     |   AP| NULL|\n",
            "| NULL|     |Blore|\n",
            "+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark doesn’t treat empty strings \"\" as null, so you need to explicitly handle them.**"
      ],
      "metadata": {
        "id": "h7G8Mxqgygdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data193 = data192.select(\n",
        "    [ when( (col(i)=='') ,None).otherwise(col(i)).alias(i) for i in data192.columns]\n",
        ")\n",
        "data193.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YntP-NoGkfWY",
        "outputId": "90b2afe6-52c7-45d8-8045-6dfe673ad856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+\n",
            "|City1|City2|City3|\n",
            "+-----+-----+-----+\n",
            "|  Goa| NULL|   AP|\n",
            "| NULL|   AP| NULL|\n",
            "| NULL| NULL|Blore|\n",
            "+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce\n",
        "data193.select(coalesce(\"City1\",\"City2\",\"City3\").alias(\"Result\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_3846EjxeAA",
        "outputId": "73f8dd70-81bf-4359-9871-c5e12c19791f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Result|\n",
            "+------+\n",
            "|   Goa|\n",
            "|    AP|\n",
            "| Blore|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. Parsing Json file**"
      ],
      "metadata": {
        "id": "Q1F40Uw8kFxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,col\n",
        "spark = SparkSession.builder.appName(\"App20\").getOrCreate()\n",
        "data200=[('John Doe','{\"street\": \"123 Main St\", \"city\": \"Anytown\"}'),('Jane Smith','{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}')]\n",
        "schema200=\"name string,address string\"\n",
        "data20 = spark.createDataFrame(data200,schema200)\n",
        "data20.show()"
      ],
      "metadata": {
        "id": "NMQpaGW7zfqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a9fe38-b36d-4eaf-f140-5204bbc23c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|      name|             address|\n",
            "+----------+--------------------+\n",
            "|  John Doe|{\"street\": \"123 M...|\n",
            "|Jane Smith|{\"street\": \"456 E...|\n",
            "+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,split\n",
        "data201 = data20.select(\"name\",split(col(\"address\"),'[{},]').alias(\"Splits\"))\n",
        "data201.show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsXuHP2glDnj",
        "outputId": "213168da-b1d9-4d5b-982f-42d5b46ffba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------------------------------+\n",
            "|name      |Splits                                            |\n",
            "+----------+--------------------------------------------------+\n",
            "|John Doe  |[, \"street\": \"123 Main St\",  \"city\": \"Anytown\", ] |\n",
            "|Jane Smith|[, \"street\": \"456 Elm St\",  \"city\": \"Othertown\", ]|\n",
            "+----------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse the json format"
      ],
      "metadata": {
        "id": "r1VRV55CoxSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "data202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\n",
        "data202.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha5vQ2_IlwUk",
        "outputId": "f4151771-1146-4528-a238-ac107d859eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------+\n",
            "|name      |Address                |\n",
            "+----------+-----------------------+\n",
            "|John Doe  |{123 Main St, Anytown} |\n",
            "|Jane Smith|{456 Elm St, Othertown}|\n",
            "+----------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2hEn82noHlV",
        "outputId": "728a900b-d0f3-49ba-cef3-05dc5e9dfdd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+---------+\n",
            "|name      |Street     |City     |\n",
            "+----------+-----------+---------+\n",
            "|John Doe  |123 Main St|Anytown  |\n",
            "|Jane Smith|456 Elm St |Othertown|\n",
            "+----------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. Get all the dataframes associated in the notebook and also display them**"
      ],
      "metadata": {
        "id": "7rGvN60Bqw-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "print(globals())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94mudGRBqWmK",
        "outputId": "3d96f827-f472-41f7-97a1-5f4f2d3310c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",struct(col(\"address\"))).show()', 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)', 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)', 'print(global)', 'print(globals())', 'from os import truncate\\nprint(globals(truncate = False))', 'from os import truncate\\nprint(globals())'], '_oh': {}, '_dh': ['/content'], 'In': ['', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",struct(col(\"address\"))).show()', 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)', 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)', 'print(global)', 'print(globals())', 'from os import truncate\\nprint(globals(truncate = False))', 'from os import truncate\\nprint(globals())'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x790fa2e39550>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x790fa2e4cbd0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x790fa2e4cbd0>, '_': '', '__': '', '___': '', '_i': 'from os import truncate\\nprint(globals(truncate = False))', '_ii': 'print(globals())', '_iii': 'print(global)', '_i1': 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()', 'SparkSession': <class 'pyspark.sql.session.SparkSession'>, 'sum': <function sum at 0x790f90eb8040>, 'spark': <pyspark.sql.session.SparkSession object at 0x790f902f1490>, 'data200': [('John Doe', '{\"street\": \"123 Main St\", \"city\": \"Anytown\"}'), ('Jane Smith', '{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}')], 'schema200': 'name string,address string', '_i2': 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', '_i3': 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20': DataFrame[name: string, address: string], '_i4': 'data20.select(\"name\",col(\"address\")[0]).show()', '_i5': 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'col': <function col at 0x790f90eb6ac0>, '_i6': 'data20.select(\"name\",col(\"address\")[0]).show()', '_i7': 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'struct': <function struct at 0x790f90e95d00>, '_i8': 'data20.select(\"name\",struct(col(\"address\"))).show()', '_i9': 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)', '_i10': 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)', 'explode': <function explode at 0x790f90ea51c0>, '_i11': 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', '_i12': 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'split': <function split at 0x790f90ed3600>, '_i13': 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)', '_i14': 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)', '_i15': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()', 'data201': DataFrame[name: string, Splits: array<string>], '_i16': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', '_i17': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show', '_i18': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', '_i19': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)', '_i20': 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)', '_i21': 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()', '_i22': 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()', 'from_json': <function from_json at 0x790f90ea5bc0>, '_i23': 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)', '_i24': 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)', '_i25': 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)', '_i26': 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)', '_i27': 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()', 'data202': DataFrame[name: string, Address: struct<street:string,city:string>], '_i28': 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)', '_i29': 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)', '_i30': 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)', '_i31': 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)', '_i32': 'print(global)', '_i33': 'print(globals())', '_i34': 'from os import truncate\\nprint(globals(truncate = False))', 'truncate': <built-in function truncate>, '_i35': 'from os import truncate\\nprint(globals())'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(globals().items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HTh54ITPrbNH",
        "outputId": "ecfdefe4-849f-4061-eb67-aa6e0798490b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dict_items([('__name__', '__main__'), ('__doc__', 'Automatically created module for IPython interactive environment'), ('__package__', None), ('__loader__', None), ('__spec__', None), ('__builtin__', <module 'builtins' (built-in)>), ('__builtins__', <module 'builtins' (built-in)>), ('_ih', ['', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",struct(col(\"address\"))).show()', 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)', 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)', 'print(global)', 'print(globals())', 'from os import truncate\\nprint(globals(truncate = False))', 'from os import truncate\\nprint(globals())', 'globals().items().show()', 'display(globals().items())']), ('_oh', {}), ('_dh', ['/content']), ('In', ['', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",col(\"address\")[0]).show()', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()', 'data20.select(\"name\",struct(col(\"address\"))).show()', 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)', 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)', 'print(global)', 'print(globals())', 'from os import truncate\\nprint(globals(truncate = False))', 'from os import truncate\\nprint(globals())', 'globals().items().show()', 'display(globals().items())']), ('Out', {}), ('get_ipython', <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x790fa2e39550>>), ('exit', <IPython.core.autocall.ZMQExitAutocall object at 0x790fa2e4cbd0>), ('quit', <IPython.core.autocall.ZMQExitAutocall object at 0x790fa2e4cbd0>), ('_', ''), ('__', ''), ('___', ''), ('_i', 'globals().items().show()'), ('_ii', 'from os import truncate\\nprint(globals())'), ('_iii', 'from os import truncate\\nprint(globals(truncate = False))'), ('_i1', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataframe(data200,schema200)\\ndata20.show()'), ('SparkSession', <class 'pyspark.sql.session.SparkSession'>), ('sum', <function sum at 0x790f90eb8040>), ('spark', <pyspark.sql.session.SparkSession object at 0x790f902f1490>), ('data200', [('John Doe', '{\"street\": \"123 Main St\", \"city\": \"Anytown\"}'), ('Jane Smith', '{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}')]), ('schema200', 'name string,address string'), ('_i2', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.Builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()'), ('_i3', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()'), ('data20', DataFrame[name: string, address: string]), ('_i4', 'data20.select(\"name\",col(\"address\")[0]).show()'), ('_i5', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()'), ('col', <function col at 0x790f90eb6ac0>), ('_i6', 'data20.select(\"name\",col(\"address\")[0]).show()'), ('_i7', 'from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum,col,struct\\nspark = SparkSession.builder.appName(\"App20\").getOrCreate()\\ndata200=[(\\'John Doe\\',\\'{\"street\": \"123 Main St\", \"city\": \"Anytown\"}\\'),(\\'Jane Smith\\',\\'{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}\\')]\\nschema200=\"name string,address string\"\\ndata20 = spark.createDataFrame(data200,schema200)\\ndata20.show()'), ('struct', <function struct at 0x790f90e95d00>), ('_i8', 'data20.select(\"name\",struct(col(\"address\"))).show()'), ('_i9', 'data20.select(\"name\",struct(col(\"address\"))).show(truncate = False)'), ('_i10', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",explode(col(\"address\"))).show(truncate = False)'), ('explode', <function explode at 0x790f90ea51c0>), ('_i11', 'from pyspark.sql.functions import col,explode\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)'), ('_i12', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\',\\')).show(truncate = False)'), ('split', <function split at 0x790f90ed3600>), ('_i13', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'{},\\')).show(truncate = False)'), ('_i14', 'from pyspark.sql.functions import col,split\\ndata20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)'), ('_i15', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.display()'), ('data201', DataFrame[name: string, Splits: array<string>]), ('_i16', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()'), ('_i17', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show'), ('_i18', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\')).show(truncate = False)\\ndata201.show()'), ('_i19', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\'))\\ndata201.show(truncate = False)'), ('_i20', 'from pyspark.sql.functions import col,split\\ndata201 = data20.select(\"name\",split(col(\"address\"),\\'[{},]\\').alias(\"Splits\"))\\ndata201.show(truncate = False)'), ('_i21', 'data201.withColumn(\"street\",col(\"Splits\")[1]).show()'), ('_i22', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show()'), ('from_json', <function from_json at 0x790f90ea5bc0>), ('_i23', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\")).show(truncate = False)'), ('_i24', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)'), ('_i25', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),street.alias(\"Street\")).show(truncate = False)'), ('_i26', 'from pyspark.sql.functions import from_json\\ndata20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"),col(\"Address\").street.alias(\"Street\")).show(truncate = False)'), ('_i27', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\")).show(truncate = False)\\ndata202.show()'), ('data202', DataFrame[name: string, Address: struct<street:string,city:string>]), ('_i28', 'from pyspark.sql.functions import from_json\\ndata202 = data20.select(\"name\",from_json(\"address\",\"street string, city string\").alias(\"Address\"))\\ndata202.show(truncate = False)'), ('_i29', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\")).show(truncate = False)'), ('_i30', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Adress\").city.alias(\"City\")).show(truncate = False)'), ('_i31', 'data202.select(\"name\", col(\"Address\").street.alias(\"Street\"), col(\"Address\").city.alias(\"City\")).show(truncate = False)'), ('_i32', 'print(global)'), ('_i33', 'print(globals())'), ('_i34', 'from os import truncate\\nprint(globals(truncate = False))'), ('truncate', <built-in function truncate>), ('_i35', 'from os import truncate\\nprint(globals())'), ('_i36', 'globals().items().show()'), ('_i37', 'display(globals().items())')])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import DataFrame\n",
        "for key,value in globals().items():\n",
        "  if type(value)==DataFrame:\n",
        "    display(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "TUHhK7EPtZAW",
        "outputId": "b116febf-99c0-4b68-9924-1c3089ea82eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data20'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, address: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data201'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, Splits: array<string>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data202'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, Address: struct<street:string,city:string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in globals().items():\n",
        "  if isinstance(value,DataFrame):\n",
        "    display(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "fqaroKv-tt5L",
        "outputId": "9e1aeefa-b3ae-4d21-9510-d8751c208e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data20'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, address: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data201'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, Splits: array<string>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'data202'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, Address: struct<street:string,city:string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. Find missing Numbers in the DataFrame**"
      ],
      "metadata": {
        "id": "t2JIW_d8vdLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,min,max\n",
        "spark = SparkSession.builder.appName(\"App 21\").getOrCreate()\n",
        "data210 = [\n",
        " (1, ),\n",
        " (2,),\n",
        " (3,),\n",
        " (6,),\n",
        " (7,),\n",
        " (8,)]\n",
        "schema210=\"Id int\"\n",
        "data21 = spark.createDataFrame(data210,schema=schema210)\n",
        "data21.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5syhqp9CvSXI",
        "outputId": "a0cc5322-6b2d-4ae0-a57c-4bdf94c07abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| Id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data211=data21.select(min(col(\"Id\")),max(col(\"Id\")))\n",
        "data211.show()\n",
        "data212=spark.range(data211.first()[0],data211.first()[1]+1)\n",
        "data212.show()\n",
        "data213=data212.subtract(data21)\n",
        "data213.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Bj-F9Nvn76",
        "outputId": "235a6a32-ffb7-400f-9ca3-684a0bcfe1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|min(Id)|max(Id)|\n",
            "+-------+-------+\n",
            "|      1|      8|\n",
            "+-------+-------+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "+---+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  5|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. How to handle multiple delimiter in a csv file**"
      ],
      "metadata": {
        "id": "uOPzjZeVFt1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum"
      ],
      "metadata": {
        "id": "Sd0AD-XPo8Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Mul_Del\").getOrCreate()\n",
        "data22 = spark.read.format('csv')\\\n",
        ".option('header', True)\\\n",
        ".load(\"/content/data22 - Sheet1.csv\")"
      ],
      "metadata": {
        "id": "rocUcGx6F6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data22.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0BBVepNLn0s",
        "outputId": "ce284b4d-23c7-4f12-a895-7cf0438c3f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|   Id|Name|Marks|\n",
            "+----------------+\n",
            "|1|Sagar|20,30,40|\n",
            "| 2|Alex|34,32,12|\n",
            "|3|David|45,67,54|\n",
            "| 4|John|10,34,60|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "data221 = data22.select((split(col(\"Id|Name|Marks\"), '[|]').alias(\"Columns\")), col(\"Columns\")[0].alias(\"ID\"), col(\"Columns\")[1].alias(\"Name\"), col(\"Columns\")[2].alias(\"Subjects\"))\n",
        "data221.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGY_LM6tLxYQ",
        "outputId": "057ed9fb-2eda-40a1-e667-e5de9bc4cdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---+-----+--------+\n",
            "|             Columns| ID| Name|Subjects|\n",
            "+--------------------+---+-----+--------+\n",
            "|[1, Sagar, 20,30,40]|  1|Sagar|20,30,40|\n",
            "| [2, Alex, 34,32,12]|  2| Alex|34,32,12|\n",
            "|[3, David, 45,67,54]|  3|David|45,67,54|\n",
            "| [4, John, 10,34,60]|  4| John|10,34,60|\n",
            "+--------------------+---+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data222 = data221.select(\"ID\",\"Name\",split(col(\"Subjects\"), ',').alias(\"Subs\"), col('Subs')[0].alias(\"Chem\"), col('Subs')[1].alias(\"Phy\"), col('Subs')[2].alias(\"Math\")).drop(\"Subs\")\n",
        "data222.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMpE_XfSMQ5A",
        "outputId": "3ef5f391-f670-43c8-cfbc-456a4adbbb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+---+----+\n",
            "| ID| Name|Chem|Phy|Math|\n",
            "+---+-----+----+---+----+\n",
            "|  1|Sagar|  20| 30|  40|\n",
            "|  2| Alex|  34| 32|  12|\n",
            "|  3|David|  45| 67|  54|\n",
            "|  4| John|  10| 34|  60|\n",
            "+---+-----+----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. Save Multiple Columns in the DataFrame**"
      ],
      "metadata": {
        "id": "-HugJLhLUccS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data230 = [\n",
        "    (1, \"Sagar\", 23, \"Male\", 68.0),\n",
        "    (2, \"Kim\", 35, \"Female\", 90.2),\n",
        "    (3, \"Alex\", 40, \"Male\", 79.1),\n",
        "]\n",
        "schema230 = \"Id int,Name string,Age int,Gender string,Marks float\"\n",
        "data23 = spark.createDataFrame(data230, schema230)\n",
        "data23.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6bdrGUYT23U",
        "outputId": "802d253d-cdc0-40a6-9e33-23f9f31d78a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+-----+\n",
            "| Id| Name|Age|Gender|Marks|\n",
            "+---+-----+---+------+-----+\n",
            "|  1|Sagar| 23|  Male| 68.0|\n",
            "|  2|  Kim| 35|Female| 90.2|\n",
            "|  3| Alex| 40|  Male| 79.1|\n",
            "+---+-----+---+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(data23)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cPe90eG9Upou",
        "outputId": "6bef53e7-7b1d-47fe-8a7e-29ad34835900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Id: int, Name: string, Age: int, Gender: string, Marks: float]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJh-gUHHVi5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}