{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBzQjeK3CJqCDDVY/dh488",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/PySpark/blob/main/Chatgpt_Pyspark_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Question 1: Customer Transactions Analysis\n",
        "Write a PySpark program to find the first transaction date and total transaction amount for each customer.\n"
      ],
      "metadata": {
        "id": "epKfo4yAn-Vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgUrEHHenRRE",
        "outputId": "6a0b6e1b-8b10-492e-f91e-1bf0d3a0b1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"CustomerTransactions\").getOrCreate()\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-10\", amount=150.0),\n",
        "    Row(customer_id=102, transaction_date=\"2023-01-12\", amount=200.0),\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-15\", amount=100.0),\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-05\", amount=50.0),\n",
        "    Row(customer_id=102, transaction_date=\"2023-01-14\", amount=300.0)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "transactions = spark.createDataFrame(data)\n",
        "\n",
        "\n",
        "# Show input data\n",
        "transactions.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "\n",
        "# Cast transaction_date from string to date\n",
        "# to_date(column, \"yyyy-MM-dd\")     column.cast(StringType())\n",
        "transactions.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\")).show()\n",
        "transactions.select( \"*\" , col(\"transaction_date\").cast(DateType())).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga76RUXNoTrk",
        "outputId": "ac1e218e-54c3-4d65-cfda-6730fffccafe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n",
            "+-----------+----------------+------+----------------+\n",
            "|customer_id|transaction_date|amount|transaction_date|\n",
            "+-----------+----------------+------+----------------+\n",
            "|        101|      2023-01-10| 150.0|      2023-01-10|\n",
            "|        102|      2023-01-12| 200.0|      2023-01-12|\n",
            "|        101|      2023-01-15| 100.0|      2023-01-15|\n",
            "|        101|      2023-01-05|  50.0|      2023-01-05|\n",
            "|        102|      2023-01-14| 300.0|      2023-01-14|\n",
            "+-----------+----------------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(transactions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rpq35soFoyAq",
        "outputId": "d66175dc-8888-46a0-c544-1dadf2c4911f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[customer_id: bigint, transaction_date: string, amount: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = transactions.withColumn( \"transaction_date\" , col(\"transaction_date\").cast(DateType()))\n",
        "transactions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4_EQ44gpVn8",
        "outputId": "e5c95821-2067-4123-b4cb-f78ef68f65ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, min\n",
        "transactions.groupBy(col(\"customer_id\")).agg(min(transactions.transaction_date).alias(\"transaction_date\"), sum(\"amount\").alias(\"amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjtPRxyWrNZt",
        "outputId": "555aeb72-dfbd-48d8-d190-b53889611ed5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-05| 300.0|\n",
            "|        102|      2023-01-12| 500.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Question Active User Streaks\n",
        "For each user, find the longest streak of consecutive login days.\n",
        "\n"
      ],
      "metadata": {
        "id": "xHpnMpeLt2sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "# Sample login data\n",
        "login_data = [\n",
        "    Row(user_id=1, login_date=\"2023-01-01\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-02\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-04\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-05\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-06\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-01\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-03\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-04\")\n",
        "]\n",
        "\n",
        "# Create the DataFrame\n",
        "logins = spark.createDataFrame(login_data)\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1wVlK6bsXl_",
        "outputId": "e257f630-09f3-4ecf-9e2b-a83f1fb77234"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|login_date|\n",
            "+-------+----------+\n",
            "|      1|2023-01-01|\n",
            "|      1|2023-01-02|\n",
            "|      1|2023-01-04|\n",
            "|      1|2023-01-05|\n",
            "|      1|2023-01-06|\n",
            "|      2|2023-01-01|\n",
            "|      2|2023-01-03|\n",
            "|      2|2023-01-04|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(logins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4ZNL_63ZzU_e",
        "outputId": "11de85ca-df0e-4ab1-8a8c-bd51bf0a9820"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, login_date: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "logins = logins.withColumn(\"login_date\", col(\"login_date\").cast(DateType()))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuNaWhrSzXd9",
        "outputId": "4b5bd1fb-dba7-4824-f12d-9d1d2d905b1b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|login_date|\n",
            "+-------+----------+\n",
            "|      1|2023-01-01|\n",
            "|      1|2023-01-02|\n",
            "|      1|2023-01-04|\n",
            "|      1|2023-01-05|\n",
            "|      1|2023-01-06|\n",
            "|      2|2023-01-01|\n",
            "|      2|2023-01-03|\n",
            "|      2|2023-01-04|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(logins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "t9KcIbQCzmQi",
        "outputId": "2ae0b51c-70d8-4140-e80d-37ddc284b1b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, login_date: date]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "have to use row_number() which is window function so import window from window\n"
      ],
      "metadata": {
        "id": "AbE1e5Ec0Aub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dense_rank cannot be used - it will over count the entries."
      ],
      "metadata": {
        "id": "k5bcAQuE9CFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "window_row = Window.partitionBy(col(\"user_id\")).orderBy(col(\"login_date\"))\n",
        "\n",
        "logins = logins.withColumn(\"row_num\", row_number().over(window_row))\n",
        "logins.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14Rtafjgzz2X",
        "outputId": "35931416-5fbc-4388-8bc8-a2ea3d1a996e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+\n",
            "|user_id|login_date|row_num|\n",
            "+-------+----------+-------+\n",
            "|      1|2023-01-01|      1|\n",
            "|      1|2023-01-02|      2|\n",
            "|      1|2023-01-04|      3|\n",
            "|      1|2023-01-05|      4|\n",
            "|      1|2023-01-06|      5|\n",
            "|      2|2023-01-01|      1|\n",
            "|      2|2023-01-03|      2|\n",
            "|      2|2023-01-04|      3|\n",
            "+-------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "error because login_date and row_num are different datatype."
      ],
      "metadata": {
        "id": "ioq0Ivv23q0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_diff\n",
        "logins = logins.withColumn(\"Difference\", date_diff((\"login_date\"),col(\"row_num\").cast(\"int\")))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "6tb_tjvp1EZ8",
        "outputId": "8ad03815-3e54-48e6-9390-0f40a58bef6d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_diff(login_date, CAST(row_num AS INT))\" due to data type mismatch: Parameter 2 requires the \"DATE\" type, however \"CAST(row_num AS INT)\" has the type \"INT\".;\n'Project [user_id#227L, login_date#240, row_num#255, date_diff(login_date#240, cast(row_num#255 as int)) AS Difference#275]\n+- Project [user_id#227L, login_date#240, row_num#255]\n   +- Project [user_id#227L, login_date#240, row_num#255, row_num#255]\n      +- Window [row_number() windowspecdefinition(user_id#227L, login_date#240 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#255], [user_id#227L], [login_date#240 ASC NULLS FIRST]\n         +- Project [user_id#227L, login_date#240]\n            +- Project [user_id#227L, cast(login_date#228 as date) AS login_date#240]\n               +- LogicalRDD [user_id#227L, login_date#228], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-3056416787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Difference\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"login_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row_num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_diff(login_date, CAST(row_num AS INT))\" due to data type mismatch: Parameter 2 requires the \"DATE\" type, however \"CAST(row_num AS INT)\" has the type \"INT\".;\n'Project [user_id#227L, login_date#240, row_num#255, date_diff(login_date#240, cast(row_num#255 as int)) AS Difference#275]\n+- Project [user_id#227L, login_date#240, row_num#255]\n   +- Project [user_id#227L, login_date#240, row_num#255, row_num#255]\n      +- Window [row_number() windowspecdefinition(user_id#227L, login_date#240 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#255], [user_id#227L], [login_date#240 ASC NULLS FIRST]\n         +- Project [user_id#227L, login_date#240]\n            +- Project [user_id#227L, cast(login_date#228 as date) AS login_date#240]\n               +- LogicalRDD [user_id#227L, login_date#228], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "expr(\"  \") --- sql like operations directly on dataframe"
      ],
      "metadata": {
        "id": "r7rMJOev4jth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_diff, expr\n",
        "logins = logins.withColumn(\"Difference\", expr(\"date_sub(login_date,row_num)\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ9FZU001yah",
        "outputId": "c6107f69-b481-47ef-d59c-3b96540bc309"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+----------+\n",
            "|user_id|login_date|row_num|Difference|\n",
            "+-------+----------+-------+----------+\n",
            "|      1|2023-01-01|      1|2022-12-31|\n",
            "|      1|2023-01-02|      2|2022-12-31|\n",
            "|      1|2023-01-04|      3|2023-01-01|\n",
            "|      1|2023-01-05|      4|2023-01-01|\n",
            "|      1|2023-01-06|      5|2023-01-01|\n",
            "|      2|2023-01-01|      1|2022-12-31|\n",
            "|      2|2023-01-03|      2|2023-01-01|\n",
            "|      2|2023-01-04|      3|2023-01-01|\n",
            "+-------+----------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "logins = logins.groupBy(col(\"user_id\"),col(\"Difference\")).agg(count(\"Difference\").alias(\"Streak\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDe4BjtT4LNC",
        "outputId": "9eeebf90-87b1-4fda-bfc4-2aff5dd2aa2a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|user_id|Difference|Streak|\n",
            "+-------+----------+------+\n",
            "|      1|2022-12-31|     2|\n",
            "|      1|2023-01-01|     3|\n",
            "|      2|2022-12-31|     1|\n",
            "|      2|2023-01-01|     2|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "logins = logins.groupBy(\"user_id\").agg((max(\"Streak\")).alias(\"Max_Streak\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPgCF87R5aG2",
        "outputId": "7c0fedc3-eef4-4c1e-bbb0-14622dac5af2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|Max_Streak|\n",
            "+-------+----------+\n",
            "|      1|         3|\n",
            "|      2|         2|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEQLfI5855EO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}