{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRyWH6P78uF1hoJf9gByNi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/PySpark/blob/main/Chatgpt_Pyspark_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Question 1: Customer Transactions Analysis\n",
        "Write a PySpark program to find the first transaction date and total transaction amount for each customer.\n"
      ],
      "metadata": {
        "id": "epKfo4yAn-Vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgUrEHHenRRE",
        "outputId": "6a0b6e1b-8b10-492e-f91e-1bf0d3a0b1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"CustomerTransactions\").getOrCreate()\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-10\", amount=150.0),\n",
        "    Row(customer_id=102, transaction_date=\"2023-01-12\", amount=200.0),\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-15\", amount=100.0),\n",
        "    Row(customer_id=101, transaction_date=\"2023-01-05\", amount=50.0),\n",
        "    Row(customer_id=102, transaction_date=\"2023-01-14\", amount=300.0)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "transactions = spark.createDataFrame(data)\n",
        "\n",
        "\n",
        "# Show input data\n",
        "transactions.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "\n",
        "# Cast transaction_date from string to date\n",
        "# to_date(column, \"yyyy-MM-dd\")     column.cast(StringType())\n",
        "transactions.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\")).show()\n",
        "transactions.select( \"*\" , col(\"transaction_date\").cast(DateType())).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga76RUXNoTrk",
        "outputId": "ac1e218e-54c3-4d65-cfda-6730fffccafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n",
            "+-----------+----------------+------+----------------+\n",
            "|customer_id|transaction_date|amount|transaction_date|\n",
            "+-----------+----------------+------+----------------+\n",
            "|        101|      2023-01-10| 150.0|      2023-01-10|\n",
            "|        102|      2023-01-12| 200.0|      2023-01-12|\n",
            "|        101|      2023-01-15| 100.0|      2023-01-15|\n",
            "|        101|      2023-01-05|  50.0|      2023-01-05|\n",
            "|        102|      2023-01-14| 300.0|      2023-01-14|\n",
            "+-----------+----------------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(transactions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rpq35soFoyAq",
        "outputId": "d66175dc-8888-46a0-c544-1dadf2c4911f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[customer_id: bigint, transaction_date: string, amount: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = transactions.withColumn( \"transaction_date\" , col(\"transaction_date\").cast(DateType()))\n",
        "transactions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4_EQ44gpVn8",
        "outputId": "e5c95821-2067-4123-b4cb-f78ef68f65ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-10| 150.0|\n",
            "|        102|      2023-01-12| 200.0|\n",
            "|        101|      2023-01-15| 100.0|\n",
            "|        101|      2023-01-05|  50.0|\n",
            "|        102|      2023-01-14| 300.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, min\n",
        "transactions.groupBy(col(\"customer_id\")).agg(min(transactions.transaction_date).alias(\"transaction_date\"), sum(\"amount\").alias(\"amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjtPRxyWrNZt",
        "outputId": "555aeb72-dfbd-48d8-d190-b53889611ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|        101|      2023-01-05| 300.0|\n",
            "|        102|      2023-01-12| 500.0|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Question Active User Streaks\n",
        "For each user, find the longest streak of consecutive login days.\n",
        "\n"
      ],
      "metadata": {
        "id": "xHpnMpeLt2sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "# Sample login data\n",
        "login_data = [\n",
        "    Row(user_id=1, login_date=\"2023-01-01\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-02\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-04\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-05\"),\n",
        "    Row(user_id=1, login_date=\"2023-01-06\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-01\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-03\"),\n",
        "    Row(user_id=2, login_date=\"2023-01-04\")\n",
        "]\n",
        "\n",
        "# Create the DataFrame\n",
        "logins = spark.createDataFrame(login_data)\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1wVlK6bsXl_",
        "outputId": "e257f630-09f3-4ecf-9e2b-a83f1fb77234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|login_date|\n",
            "+-------+----------+\n",
            "|      1|2023-01-01|\n",
            "|      1|2023-01-02|\n",
            "|      1|2023-01-04|\n",
            "|      1|2023-01-05|\n",
            "|      1|2023-01-06|\n",
            "|      2|2023-01-01|\n",
            "|      2|2023-01-03|\n",
            "|      2|2023-01-04|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(logins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4ZNL_63ZzU_e",
        "outputId": "11de85ca-df0e-4ab1-8a8c-bd51bf0a9820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, login_date: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "logins = logins.withColumn(\"login_date\", col(\"login_date\").cast(DateType()))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuNaWhrSzXd9",
        "outputId": "4b5bd1fb-dba7-4824-f12d-9d1d2d905b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|login_date|\n",
            "+-------+----------+\n",
            "|      1|2023-01-01|\n",
            "|      1|2023-01-02|\n",
            "|      1|2023-01-04|\n",
            "|      1|2023-01-05|\n",
            "|      1|2023-01-06|\n",
            "|      2|2023-01-01|\n",
            "|      2|2023-01-03|\n",
            "|      2|2023-01-04|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(logins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "t9KcIbQCzmQi",
        "outputId": "2ae0b51c-70d8-4140-e80d-37ddc284b1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, login_date: date]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "have to use row_number() which is window function so import window from window\n"
      ],
      "metadata": {
        "id": "AbE1e5Ec0Aub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dense_rank cannot be used - it will over count the entries."
      ],
      "metadata": {
        "id": "k5bcAQuE9CFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "window_row = Window.partitionBy(col(\"user_id\")).orderBy(col(\"login_date\"))\n",
        "\n",
        "logins = logins.withColumn(\"row_num\", row_number().over(window_row))\n",
        "logins.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14Rtafjgzz2X",
        "outputId": "35931416-5fbc-4388-8bc8-a2ea3d1a996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+\n",
            "|user_id|login_date|row_num|\n",
            "+-------+----------+-------+\n",
            "|      1|2023-01-01|      1|\n",
            "|      1|2023-01-02|      2|\n",
            "|      1|2023-01-04|      3|\n",
            "|      1|2023-01-05|      4|\n",
            "|      1|2023-01-06|      5|\n",
            "|      2|2023-01-01|      1|\n",
            "|      2|2023-01-03|      2|\n",
            "|      2|2023-01-04|      3|\n",
            "+-------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "error because login_date and row_num are different datatype."
      ],
      "metadata": {
        "id": "ioq0Ivv23q0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_diff\n",
        "logins = logins.withColumn(\"Difference\", date_diff((\"login_date\"),col(\"row_num\").cast(\"int\")))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "6tb_tjvp1EZ8",
        "outputId": "8ad03815-3e54-48e6-9390-0f40a58bef6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_diff(login_date, CAST(row_num AS INT))\" due to data type mismatch: Parameter 2 requires the \"DATE\" type, however \"CAST(row_num AS INT)\" has the type \"INT\".;\n'Project [user_id#227L, login_date#240, row_num#255, date_diff(login_date#240, cast(row_num#255 as int)) AS Difference#275]\n+- Project [user_id#227L, login_date#240, row_num#255]\n   +- Project [user_id#227L, login_date#240, row_num#255, row_num#255]\n      +- Window [row_number() windowspecdefinition(user_id#227L, login_date#240 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#255], [user_id#227L], [login_date#240 ASC NULLS FIRST]\n         +- Project [user_id#227L, login_date#240]\n            +- Project [user_id#227L, cast(login_date#228 as date) AS login_date#240]\n               +- LogicalRDD [user_id#227L, login_date#228], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-3056416787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Difference\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"login_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row_num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_diff(login_date, CAST(row_num AS INT))\" due to data type mismatch: Parameter 2 requires the \"DATE\" type, however \"CAST(row_num AS INT)\" has the type \"INT\".;\n'Project [user_id#227L, login_date#240, row_num#255, date_diff(login_date#240, cast(row_num#255 as int)) AS Difference#275]\n+- Project [user_id#227L, login_date#240, row_num#255]\n   +- Project [user_id#227L, login_date#240, row_num#255, row_num#255]\n      +- Window [row_number() windowspecdefinition(user_id#227L, login_date#240 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#255], [user_id#227L], [login_date#240 ASC NULLS FIRST]\n         +- Project [user_id#227L, login_date#240]\n            +- Project [user_id#227L, cast(login_date#228 as date) AS login_date#240]\n               +- LogicalRDD [user_id#227L, login_date#228], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "expr(\"  \") --- sql like operations directly on dataframe"
      ],
      "metadata": {
        "id": "r7rMJOev4jth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_diff, expr\n",
        "logins = logins.withColumn(\"Difference\", expr(\"date_sub(login_date,row_num)\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ9FZU001yah",
        "outputId": "c6107f69-b481-47ef-d59c-3b96540bc309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+----------+\n",
            "|user_id|login_date|row_num|Difference|\n",
            "+-------+----------+-------+----------+\n",
            "|      1|2023-01-01|      1|2022-12-31|\n",
            "|      1|2023-01-02|      2|2022-12-31|\n",
            "|      1|2023-01-04|      3|2023-01-01|\n",
            "|      1|2023-01-05|      4|2023-01-01|\n",
            "|      1|2023-01-06|      5|2023-01-01|\n",
            "|      2|2023-01-01|      1|2022-12-31|\n",
            "|      2|2023-01-03|      2|2023-01-01|\n",
            "|      2|2023-01-04|      3|2023-01-01|\n",
            "+-------+----------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "logins = logins.groupBy(col(\"user_id\"),col(\"Difference\")).agg(count(\"Difference\").alias(\"Streak\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDe4BjtT4LNC",
        "outputId": "9eeebf90-87b1-4fda-bfc4-2aff5dd2aa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|user_id|Difference|Streak|\n",
            "+-------+----------+------+\n",
            "|      1|2022-12-31|     2|\n",
            "|      1|2023-01-01|     3|\n",
            "|      2|2022-12-31|     1|\n",
            "|      2|2023-01-01|     2|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "logins = logins.groupBy(\"user_id\").agg((max(\"Streak\")).alias(\"Max_Streak\"))\n",
        "logins.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPgCF87R5aG2",
        "outputId": "7c0fedc3-eef4-4c1e-bbb0-14622dac5af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|user_id|Max_Streak|\n",
            "+-------+----------+\n",
            "|      1|         3|\n",
            "|      2|         2|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 3: Identify Top 2 Products by Revenue per Category"
      ],
      "metadata": {
        "id": "qNB0JTCFjrmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Start a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"TopProductsByRevenue\").getOrCreate()\n",
        "# ✅ Sample input data\n",
        "data = [\n",
        "    (1, \"A\", \"Phones\", 500, 2),\n",
        "    (2, \"B\", \"Phones\", 300, 3),\n",
        "    (3, \"C\", \"Laptops\", 1000, 1),\n",
        "    (4, \"D\", \"Phones\", 700, 1),\n",
        "    (5, \"E\", \"Laptops\", 800, 2),\n",
        "    (6, \"F\", \"Laptops\", 1000, 1),\n",
        "]\n",
        "\n",
        "columns = [\"order_id\", \"product_id\", \"category\", \"price\", \"quantity\"]\n",
        "\n",
        "orders = spark.createDataFrame(data, columns)\n",
        "orders.show()\n"
      ],
      "metadata": {
        "id": "MEQLfI5855EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d676acb-cccf-4102-809c-2aeb733bc7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+-----+--------+\n",
            "|order_id|product_id|category|price|quantity|\n",
            "+--------+----------+--------+-----+--------+\n",
            "|       1|         A|  Phones|  500|       2|\n",
            "|       2|         B|  Phones|  300|       3|\n",
            "|       3|         C| Laptops| 1000|       1|\n",
            "|       4|         D|  Phones|  700|       1|\n",
            "|       5|         E| Laptops|  800|       2|\n",
            "|       6|         F| Laptops| 1000|       1|\n",
            "+--------+----------+--------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "orders = orders.withColumn(\"total_revenue\",col(\"price\")*col(\"quantity\"))\n",
        "orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZxy5GSZkzRb",
        "outputId": "eeedbaf3-8a73-4f4d-ba1d-ce778abf0579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+-----+--------+-------------+\n",
            "|order_id|product_id|category|price|quantity|total_revenue|\n",
            "+--------+----------+--------+-----+--------+-------------+\n",
            "|       1|         A|  Phones|  500|       2|         1000|\n",
            "|       2|         B|  Phones|  300|       3|          900|\n",
            "|       3|         C| Laptops| 1000|       1|         1000|\n",
            "|       4|         D|  Phones|  700|       1|          700|\n",
            "|       5|         E| Laptops|  800|       2|         1600|\n",
            "|       6|         F| Laptops| 1000|       1|         1000|\n",
            "+--------+----------+--------+-----+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide categorically and give top 2 for each means top 2 ranks for each category hence window functions.\n",
        "\n",
        "from pyspark.sql.functions import desc, row_number\n",
        "from pyspark.sql.window import Window\n",
        "windowCat = Window.partitionBy(col(\"category\")).orderBy(desc(col(\"total_revenue\")))\n",
        "orders = orders.withColumn(\"row_num\", row_number().over(windowCat))\n",
        "orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9dIokjclRto",
        "outputId": "8fa0d69b-e6bd-4299-ed94-bd2b5d39a942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+-----+--------+-------------+-------+\n",
            "|order_id|product_id|category|price|quantity|total_revenue|row_num|\n",
            "+--------+----------+--------+-----+--------+-------------+-------+\n",
            "|       5|         E| Laptops|  800|       2|         1600|      1|\n",
            "|       3|         C| Laptops| 1000|       1|         1000|      2|\n",
            "|       6|         F| Laptops| 1000|       1|         1000|      3|\n",
            "|       1|         A|  Phones|  500|       2|         1000|      1|\n",
            "|       2|         B|  Phones|  300|       3|          900|      2|\n",
            "|       4|         D|  Phones|  700|       1|          700|      3|\n",
            "+--------+----------+--------+-----+--------+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders.select(col(\"category\"), col(\"product_id\"), col(\"total_revenue\")) \\\n",
        ".where(col(\"row_num\")<=2) \\\n",
        ".orderBy(\"category\", desc(\"total_revenue\")).show()\n",
        "\n",
        "\"\"\"\n",
        "orders.filter(col(\"row_num\") <= 2) \\\n",
        "      .select(\"category\", \"product_id\", \"total_revenue\") \\\n",
        "      .orderBy(\"category\", desc(\"total_revenue\")) \\\n",
        "      .show()\n",
        "      \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLp3pxfplsG5",
        "outputId": "8f251776-6852-4174-8a3c-d42085454d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-------------+\n",
            "|category|product_id|total_revenue|\n",
            "+--------+----------+-------------+\n",
            "| Laptops|         E|         1600|\n",
            "| Laptops|         C|         1000|\n",
            "|  Phones|         A|         1000|\n",
            "|  Phones|         B|          900|\n",
            "+--------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Compute Running Revenue per Customer"
      ],
      "metadata": {
        "id": "gIeq3aGtsuUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Sample transaction data\n",
        "data = [\n",
        "    (1, 101, \"2024-01-01\", 200),\n",
        "    (2, 101, \"2024-01-03\", 150),\n",
        "    (3, 102, \"2024-01-02\", 300),\n",
        "    (4, 101, \"2024-01-05\", 100),\n",
        "    (5, 102, \"2024-01-04\", 200),\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"transaction_date\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "transaction = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show input\n",
        "transaction.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzU_NZijonoJ",
        "outputId": "9c3ebe04-06ce-4582-948c-296811c3bf95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+------+\n",
            "|transaction_id|customer_id|transaction_date|amount|\n",
            "+--------------+-----------+----------------+------+\n",
            "|             1|        101|      2024-01-01|   200|\n",
            "|             2|        101|      2024-01-03|   150|\n",
            "|             3|        102|      2024-01-02|   300|\n",
            "|             4|        101|      2024-01-05|   100|\n",
            "|             5|        102|      2024-01-04|   200|\n",
            "+--------------+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(transaction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "5zrXHeKXs3sk",
        "outputId": "1952cc89-9534-4801-b3cb-5e1e3767e89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[transaction_id: int, customer_id: int, transaction_date: string, amount: int]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "transaction = transaction.withColumn(\"transaction_date\", to_date(\"transaction_date\",'yyyy-MM-dd'))\n",
        "transaction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLshceAws6mb",
        "outputId": "a45a7f82-f486-40d6-bb27-60a4a2ecdca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+------+\n",
            "|transaction_id|customer_id|transaction_date|amount|\n",
            "+--------------+-----------+----------------+------+\n",
            "|             1|        101|      2024-01-01|   200|\n",
            "|             2|        101|      2024-01-03|   150|\n",
            "|             3|        102|      2024-01-02|   300|\n",
            "|             4|        101|      2024-01-05|   100|\n",
            "|             5|        102|      2024-01-04|   200|\n",
            "+--------------+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ❌ Incorrect logic:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "transaction = transaction.withColumn(\"row_num\", row_number().over(window_cus))\n",
        "row_number() just gives you the position of the row, not the cumulative sum. However, to compute the running total of amount (as per the question), we need to use sum() as a window function, not row_number().\n"
      ],
      "metadata": {
        "id": "TgSmRH9QvCbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_cus = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
        "transaction = transaction.withColumn(\"row_num\",row_number().over(window_cus))\n",
        "transaction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_NeGcO5tUIA",
        "outputId": "3591fa6c-fb5f-4c09-99a1-1d17cc2df94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+------+-------+\n",
            "|transaction_id|customer_id|transaction_date|amount|row_num|\n",
            "+--------------+-----------+----------------+------+-------+\n",
            "|             1|        101|      2024-01-01|   200|      1|\n",
            "|             2|        101|      2024-01-03|   150|      2|\n",
            "|             4|        101|      2024-01-05|   100|      3|\n",
            "|             3|        102|      2024-01-02|   300|      1|\n",
            "|             5|        102|      2024-01-04|   200|      2|\n",
            "+--------------+-----------+----------------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sum in window giving cumulative\n",
        "from pyspark.sql.functions import sum\n",
        "window_cus = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
        "transaction = transaction.withColumn(\"running_tot\" ,sum(\"amount\").over(window_cus))\n",
        "transaction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfT5lyCRvNjn",
        "outputId": "a6b9b289-e951-4b25-ca50-3aaa90efea96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+------+-------+-----------+\n",
            "|transaction_id|customer_id|transaction_date|amount|row_num|running_tot|\n",
            "+--------------+-----------+----------------+------+-------+-----------+\n",
            "|             1|        101|      2024-01-01|   200|      1|        200|\n",
            "|             2|        101|      2024-01-03|   150|      2|        350|\n",
            "|             4|        101|      2024-01-05|   100|      3|        450|\n",
            "|             3|        102|      2024-01-02|   300|      1|        300|\n",
            "|             5|        102|      2024-01-04|   200|      2|        500|\n",
            "+--------------+-----------+----------------+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(transaction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hGQwN7FiuM2a",
        "outputId": "e4dc82c7-1446-4c22-a4eb-184a60bab75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[transaction_id: int, customer_id: int, transaction_date: date, amount: int, row_num: int]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_cus = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\") \\\n",
        "                   .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "'''What does .rowsBetween(Window.unboundedPreceding, Window.currentRow) mean?\n",
        "Window.unboundedPreceding: start from the first row in the partition\n",
        "\n",
        "Window.currentRow: include up to the current row\n",
        "➡️ So this includes all rows up to and including the current row, which is exactly what you want for a cumulative or running sum.'''"
      ],
      "metadata": {
        "id": "4IvsGGnov1CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 — User Purchase Frequency Change\n",
        "For each user, find the number of days between consecutive purchases and the difference in amount spent."
      ],
      "metadata": {
        "id": "nNsEPt5kyiHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"purchase_date\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Step 4: Create sample data\n",
        "data = [\n",
        "    (1, \"2024-01-01\", 100),\n",
        "    (1, \"2024-01-15\", 200),\n",
        "    (2, \"2024-01-03\", 150),\n",
        "    (1, \"2024-02-01\", 300),\n",
        "    (2, \"2024-01-20\", 100),\n",
        "    (3, \"2024-01-10\", 400),\n",
        "    (3, \"2024-02-11\", 200)\n",
        "]\n",
        "\n",
        "# Step 5: Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Step 6: Convert string to date type\n",
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn(\"purchase_date\", to_date(col(\"purchase_date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Step 7: Show the input\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-pROrLQynQd",
        "outputId": "b0474773-214b-4609-865c-57093d56521b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+------+\n",
            "|user_id|purchase_date|amount|\n",
            "+-------+-------------+------+\n",
            "|      1|   2024-01-01|   100|\n",
            "|      1|   2024-01-15|   200|\n",
            "|      2|   2024-01-03|   150|\n",
            "|      1|   2024-02-01|   300|\n",
            "|      2|   2024-01-20|   100|\n",
            "|      3|   2024-01-10|   400|\n",
            "|      3|   2024-02-11|   200|\n",
            "+-------+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For each user, find the number of days between consecutive purchases and the difference in amount spent.\n",
        "# So we need to find diff b/w previous and current  days, and purchase\n",
        "# to get previous use lag\n",
        "\n",
        "from pyspark.sql.functions import lag\n",
        "window_lag = Window.partitionBy(\"user_id\").orderBy(\"purchase_date\")\n",
        "\n",
        "df = df.withColumn(\"previous_date\", lag(\"purchase_date\").over(window_lag)) \\\n",
        "       .withColumn(\"previous_amount\", lag(\"amount\").over(window_lag))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vU_zzWV0Zkk",
        "outputId": "a530cf0c-aa89-4add-f87a-6d220bf5eb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+------+-------------+---------------+\n",
            "|user_id|purchase_date|amount|previous_date|previous_amount|\n",
            "+-------+-------------+------+-------------+---------------+\n",
            "|      1|   2024-01-01|   100|         NULL|           NULL|\n",
            "|      1|   2024-01-15|   200|   2024-01-01|            100|\n",
            "|      1|   2024-02-01|   300|   2024-01-15|            200|\n",
            "|      2|   2024-01-03|   150|         NULL|           NULL|\n",
            "|      2|   2024-01-20|   100|   2024-01-03|            150|\n",
            "|      3|   2024-01-10|   400|         NULL|           NULL|\n",
            "|      3|   2024-02-11|   200|   2024-01-10|            400|\n",
            "+-------+-------------+------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "QrpTlQj39eRd",
        "outputId": "b9b9649c-7b45-420c-a5a5-25595ad072c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: int, purchase_date: date, amount: int, previous_date: date, previous_amount: int]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating diff\n",
        "from pyspark.sql.functions import date_diff\n",
        "df = df.withColumn(\"Num_days\",date_diff(\"purchase_date\",\"previous_date\")).withColumn(\"Amount_diff\",col(\"amount\")-col(\"previous_amount\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3OUNSLq8iDJ",
        "outputId": "2b5e6cee-f9a6-469d-aa8b-5597c677cfe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+------+-------------+---------------+--------+-----------+\n",
            "|user_id|purchase_date|amount|previous_date|previous_amount|Num_days|Amount_diff|\n",
            "+-------+-------------+------+-------------+---------------+--------+-----------+\n",
            "|      1|   2024-01-01|   100|         NULL|           NULL|    NULL|       NULL|\n",
            "|      1|   2024-01-15|   200|   2024-01-01|            100|      14|        100|\n",
            "|      1|   2024-02-01|   300|   2024-01-15|            200|      17|        100|\n",
            "|      2|   2024-01-03|   150|         NULL|           NULL|    NULL|       NULL|\n",
            "|      2|   2024-01-20|   100|   2024-01-03|            150|      17|        -50|\n",
            "|      3|   2024-01-10|   400|         NULL|           NULL|    NULL|       NULL|\n",
            "|      3|   2024-02-11|   200|   2024-01-10|            400|      32|       -200|\n",
            "+-------+-------------+------+-------------+---------------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"user_id\",\"purchase_date\",\"amount\",\"Num_days\",\"Amount_diff\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sLo2EBU-VS3",
        "outputId": "172a1725-d6ef-40a3-e015-22d1e81bec60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+------+--------+-----------+\n",
            "|user_id|purchase_date|amount|Num_days|Amount_diff|\n",
            "+-------+-------------+------+--------+-----------+\n",
            "|      1|   2024-01-01|   100|    NULL|       NULL|\n",
            "|      1|   2024-01-15|   200|      14|        100|\n",
            "|      1|   2024-02-01|   300|      17|        100|\n",
            "|      2|   2024-01-03|   150|    NULL|       NULL|\n",
            "|      2|   2024-01-20|   100|      17|        -50|\n",
            "|      3|   2024-01-10|   400|    NULL|       NULL|\n",
            "|      3|   2024-02-11|   200|      32|       -200|\n",
            "+-------+-------------+------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 6: Repeat Purchasers Within 30 Days\n",
        " You’re given a transactions dataset with customer purchases. Find customers who made more than one purchase within a 30-day window."
      ],
      "metadata": {
        "id": "MMuTPpXh_qTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.drop()\n",
        "schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"transaction_date\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Step 5: Create sample data\n",
        "data = [\n",
        "    (1, \"2024-01-01\", 100),\n",
        "    (1, \"2024-01-20\", 200),\n",
        "    (1, \"2024-03-01\", 300),\n",
        "    (2, \"2024-01-01\", 150),\n",
        "    (2, \"2024-02-10\", 100)\n",
        "]\n",
        "\n",
        "# Step 6: Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Step 7: Convert transaction_date to DateType\n",
        "df = df.withColumn(\"transaction_date\", to_date(\"transaction_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Step 8: Show input data\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aGsqkRa_gPL",
        "outputId": "d1ff3173-94f9-4ca1-fc6c-5d288311314d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|transaction_date|amount|\n",
            "+-----------+----------------+------+\n",
            "|          1|      2024-01-01|   100|\n",
            "|          1|      2024-01-20|   200|\n",
            "|          1|      2024-03-01|   300|\n",
            "|          2|      2024-01-01|   150|\n",
            "|          2|      2024-02-10|   100|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# previous purchase using lag\n",
        "from pyspark.sql.functions import lead\n",
        "window_30lead = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
        "\n",
        "df = df.withColumn(\"second_transaction_date\", lead(\"transaction_date\").over(window_30lead))\\\n",
        "      .withColumnRenamed(\"transaction_date\",\"first_transaction_date\") \\\n",
        "      .drop(\"amount\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQn9JxeBroX",
        "outputId": "95293bd2-0161-465d-e498-5875a5c71de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------------+-----------------------+\n",
            "|customer_id|first_transaction_date|second_transaction_date|\n",
            "+-----------+----------------------+-----------------------+\n",
            "|          1|            2024-01-01|             2024-01-20|\n",
            "|          1|            2024-01-20|             2024-03-01|\n",
            "|          1|            2024-03-01|                   NULL|\n",
            "|          2|            2024-01-01|             2024-02-10|\n",
            "|          2|            2024-02-10|                   NULL|\n",
            "+-----------+----------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating days diff\n",
        "\n",
        "df = df.withColumn(\"Days_diff\",date_diff(\"second_transaction_date\",\"first_transaction_date\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsmczTskCZUW",
        "outputId": "897b66a6-d346-4990-b40f-664a058f7d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------------+-----------------------+---------+\n",
            "|customer_id|first_transaction_date|second_transaction_date|Days_diff|\n",
            "+-----------+----------------------+-----------------------+---------+\n",
            "|          1|            2024-01-01|             2024-01-20|       19|\n",
            "|          1|            2024-01-20|             2024-03-01|       41|\n",
            "|          1|            2024-03-01|                   NULL|     NULL|\n",
            "|          2|            2024-01-01|             2024-02-10|       40|\n",
            "|          2|            2024-02-10|                   NULL|     NULL|\n",
            "+-----------+----------------------+-----------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"Days_diff\") <= 30).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1oqlHBnCvpl",
        "outputId": "196c3d25-8b20-46eb-f552-45a79ddbc15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------------+-----------------------+---------+\n",
            "|customer_id|first_transaction_date|second_transaction_date|Days_diff|\n",
            "+-----------+----------------------+-----------------------+---------+\n",
            "|          1|            2024-01-01|             2024-01-20|       19|\n",
            "+-----------+----------------------+-----------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 7: Sessionize User Activity with Idle Timeout\n",
        "\n",
        " Group each user’s activities into sessions, where a session is defined as a series of events separated by no more than 15 minutes of inactivity.\n",
        "\n",
        "Assign a session_id to each event, such that events within the same session have the same session_id.\n",
        "\n"
      ],
      "metadata": {
        "id": "JSjuCh0w4g2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Step 3: Start Spark session\n",
        "spark = SparkSession.builder.appName(\"SessionizeUserActivity\").getOrCreate()\n",
        "\n",
        "# Step 4: Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"activity_timestamp\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Step 5: Sample data\n",
        "data = [\n",
        "    (1, \"2024-06-01 10:00:00\"),\n",
        "    (1, \"2024-06-01 10:05:00\"),\n",
        "    (1, \"2024-06-01 10:40:00\"),\n",
        "    (2, \"2024-06-01 09:00:00\"),\n",
        "    (2, \"2024-06-01 09:20:00\"),\n",
        "    (2, \"2024-06-01 09:55:00\"),\n",
        "]\n",
        "\n",
        "# Step 6: Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjXuSbPQswB1",
        "outputId": "b685b113-d159-48fe-bfdf-c8ddbd915d9d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|user_id| activity_timestamp|\n",
            "+-------+-------------------+\n",
            "|      1|2024-06-01 10:00:00|\n",
            "|      1|2024-06-01 10:05:00|\n",
            "|      1|2024-06-01 10:40:00|\n",
            "|      2|2024-06-01 09:00:00|\n",
            "|      2|2024-06-01 09:20:00|\n",
            "|      2|2024-06-01 09:55:00|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "BbABnZlptJhC",
        "outputId": "16285eb6-ed19-452c-95b9-3b2008532a7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: int, activity_timestamp: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"activity_timestamp\",to_timestamp(\"activity_timestamp\",\"yyyy-MM-dd HH:mm:ss\"))\n",
        "display(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "ZToDiLU2tqDd",
        "outputId": "c099e716-4f06-4eeb-a0ca-b68595d4b448"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: int, activity_timestamp: timestamp]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|user_id| activity_timestamp|\n",
            "+-------+-------------------+\n",
            "|      1|2024-06-01 10:00:00|\n",
            "|      1|2024-06-01 10:05:00|\n",
            "|      1|2024-06-01 10:40:00|\n",
            "|      2|2024-06-01 09:00:00|\n",
            "|      2|2024-06-01 09:20:00|\n",
            "|      2|2024-06-01 09:55:00|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag\n",
        "window_15lag = Window.partitionBy(\"user_id\").orderBy(\"activity_timestamp\")\n",
        "df = df.withColumn(\"previous_activity\",lag(\"activity_timestamp\").over(window_15lag))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvyUOg90uUa0",
        "outputId": "eb757c12-dcf3-4b1f-994f-fc2db1ad4cd8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+\n",
            "|user_id| activity_timestamp|  previous_activity|\n",
            "+-------+-------------------+-------------------+\n",
            "|      1|2024-06-01 10:00:00|               NULL|\n",
            "|      1|2024-06-01 10:05:00|2024-06-01 10:00:00|\n",
            "|      1|2024-06-01 10:40:00|2024-06-01 10:05:00|\n",
            "|      2|2024-06-01 09:00:00|               NULL|\n",
            "|      2|2024-06-01 09:20:00|2024-06-01 09:00:00|\n",
            "|      2|2024-06-01 09:55:00|2024-06-01 09:20:00|\n",
            "+-------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, col\n",
        "df = df.withColumn(\"Time_Diff\",((unix_timestamp(col(\"activity_timestamp\"))-unix_timestamp(col(\"previous_activity\"))))/60)\n",
        "df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41CWX9ebxEfF",
        "outputId": "f2bc18ff-23bd-4c85-d980-598b1de4332e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+---------+\n",
            "|user_id|activity_timestamp |previous_activity  |Time_Diff|\n",
            "+-------+-------------------+-------------------+---------+\n",
            "|1      |2024-06-01 10:00:00|NULL               |NULL     |\n",
            "|1      |2024-06-01 10:05:00|2024-06-01 10:00:00|5.0      |\n",
            "|1      |2024-06-01 10:40:00|2024-06-01 10:05:00|35.0     |\n",
            "|2      |2024-06-01 09:00:00|NULL               |NULL     |\n",
            "|2      |2024-06-01 09:20:00|2024-06-01 09:00:00|20.0     |\n",
            "|2      |2024-06-01 09:55:00|2024-06-01 09:20:00|35.0     |\n",
            "+-------+-------------------+-------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"Session_strt\", when((col(\"Time_Diff\") > 15) | (col(\"Time_Diff\").isNull()), 1).otherwise(0))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMeGOxdZxvHi",
        "outputId": "da432417-4518-4bb6-8926-7eb56aeb6325"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+---------+------------+\n",
            "|user_id| activity_timestamp|  previous_activity|Time_Diff|Session_strt|\n",
            "+-------+-------------------+-------------------+---------+------------+\n",
            "|      1|2024-06-01 10:00:00|               NULL|     NULL|           1|\n",
            "|      1|2024-06-01 10:05:00|2024-06-01 10:00:00|      5.0|           0|\n",
            "|      1|2024-06-01 10:40:00|2024-06-01 10:05:00|     35.0|           1|\n",
            "|      2|2024-06-01 09:00:00|               NULL|     NULL|           1|\n",
            "|      2|2024-06-01 09:20:00|2024-06-01 09:00:00|     20.0|           1|\n",
            "|      2|2024-06-01 09:55:00|2024-06-01 09:20:00|     35.0|           1|\n",
            "+-------+-------------------+-------------------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "df = df.withColumn(\"Session_id\", sum(\"Session_strt\").over(window_15lag) )\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z0t2auIzLeP",
        "outputId": "d75d2614-459e-4af2-cae1-391b92aab0c8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+---------+------------+----------+\n",
            "|user_id| activity_timestamp|  previous_activity|Time_Diff|Session_strt|Session_id|\n",
            "+-------+-------------------+-------------------+---------+------------+----------+\n",
            "|      1|2024-06-01 10:00:00|               NULL|     NULL|           1|         1|\n",
            "|      1|2024-06-01 10:05:00|2024-06-01 10:00:00|      5.0|           0|         1|\n",
            "|      1|2024-06-01 10:40:00|2024-06-01 10:05:00|     35.0|           1|         2|\n",
            "|      2|2024-06-01 09:00:00|               NULL|     NULL|           1|         1|\n",
            "|      2|2024-06-01 09:20:00|2024-06-01 09:00:00|     20.0|           1|         2|\n",
            "|      2|2024-06-01 09:55:00|2024-06-01 09:20:00|     35.0|           1|         3|\n",
            "+-------+-------------------+-------------------+---------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"user_id\",\"activity_timestamp\",\"Session_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxK6K08F7GEO",
        "outputId": "3ed7ac14-e173-471a-b067-de3ff068fd55"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+\n",
            "|user_id| activity_timestamp|Session_id|\n",
            "+-------+-------------------+----------+\n",
            "|      1|2024-06-01 10:00:00|         1|\n",
            "|      1|2024-06-01 10:05:00|         1|\n",
            "|      1|2024-06-01 10:40:00|         2|\n",
            "|      2|2024-06-01 09:00:00|         1|\n",
            "|      2|2024-06-01 09:20:00|         2|\n",
            "|      2|2024-06-01 09:55:00|         3|\n",
            "+-------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 : Calculate Rolling 3-Day Revenue per User\n",
        "You are given a transactions dataset. For each user and each transaction date, compute the rolling total revenue for that user over the past 3 days, including the current day."
      ],
      "metadata": {
        "id": "ZoQ4-eTaJjyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\n",
        "    (1, \"2024-06-01\", 100),\n",
        "    (1, \"2024-06-02\", 200),\n",
        "    (1, \"2024-06-03\", 300),\n",
        "    (1, \"2024-06-05\", 400),\n",
        "    (2, \"2024-06-01\", 150),\n",
        "    (2, \"2024-06-04\", 250),\n",
        "    (2, \"2024-06-05\", 350),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"user_id\", \"transaction_date\", \"revenue\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sKi7AVZ7cOM",
        "outputId": "7459c469-1175-491c-edc1-27621ce4cf30"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------+\n",
            "|user_id|transaction_date|revenue|\n",
            "+-------+----------------+-------+\n",
            "|      1|      2024-06-01|    100|\n",
            "|      1|      2024-06-02|    200|\n",
            "|      1|      2024-06-03|    300|\n",
            "|      1|      2024-06-05|    400|\n",
            "|      2|      2024-06-01|    150|\n",
            "|      2|      2024-06-04|    250|\n",
            "|      2|      2024-06-05|    350|\n",
            "+-------+----------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "RSQ0YRenJyrD",
        "outputId": "ec90c653-8712-4555-efcd-ab287f41abb2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, transaction_date: string, revenue: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "df = df.withColumn(\"transaction_date\",col(\"transaction_date\").cast(DateType()))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzPSu5moJ1-b",
        "outputId": "9ab8d8f3-ed4b-4e00-bd6c-4334bd20dd53"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------+\n",
            "|user_id|transaction_date|revenue|\n",
            "+-------+----------------+-------+\n",
            "|      1|      2024-06-01|    100|\n",
            "|      1|      2024-06-02|    200|\n",
            "|      1|      2024-06-03|    300|\n",
            "|      1|      2024-06-05|    400|\n",
            "|      2|      2024-06-01|    150|\n",
            "|      2|      2024-06-04|    250|\n",
            "|      2|      2024-06-05|    350|\n",
            "+-------+----------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hLdH7pemKKP-",
        "outputId": "cc251dd2-6cbb-4d93-a7a0-c0b3689e346a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, transaction_date: date, revenue: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_sub\n",
        "window_3lag = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\").rangeBetween(-3*86400 + 1,0)\n",
        "df.withColumn(\" \",sum(\"revenue\").over(window_3lag)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "LGWABltHKPQ0",
        "outputId": "4c0d1c91-3848-4993-b1c5-783f4736a70d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[DATATYPE_MISMATCH.RANGE_FRAME_INVALID_TYPE] Cannot resolve \"(PARTITION BY user_id ORDER BY transaction_date ASC NULLS FIRST RANGE BETWEEN -259199 FOLLOWING AND CURRENT ROW)\" due to data type mismatch: The data type \"DATE\" used in the order specification does not match the data type \"BIGINT\" which is used in the range frame.;\n'Project [user_id#908L, transaction_date#927, revenue#910L, sum(revenue#910L) windowspecdefinition(user_id#908L, transaction_date#927 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -259199, currentrow$())) AS  #972]\n+- Project [user_id#908L, cast(transaction_date#909 as date) AS transaction_date#927, revenue#910L]\n   +- LogicalRDD [user_id#908L, transaction_date#909, revenue#910L], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-81-1945210835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate_sub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwindow_3lag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transaction_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrangeBetween\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m86400\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revenue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_3lag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.RANGE_FRAME_INVALID_TYPE] Cannot resolve \"(PARTITION BY user_id ORDER BY transaction_date ASC NULLS FIRST RANGE BETWEEN -259199 FOLLOWING AND CURRENT ROW)\" due to data type mismatch: The data type \"DATE\" used in the order specification does not match the data type \"BIGINT\" which is used in the range frame.;\n'Project [user_id#908L, transaction_date#927, revenue#910L, sum(revenue#910L) windowspecdefinition(user_id#908L, transaction_date#927 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -259199, currentrow$())) AS  #972]\n+- Project [user_id#908L, cast(transaction_date#909 as date) AS transaction_date#927, revenue#910L]\n   +- LogicalRDD [user_id#908L, transaction_date#909, revenue#910L], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".orderBy(\"transaction_date\").rangeBetween(-3*86400 + 1,0)\n",
        "here the range Between is applied on orderBy column and also it takes only numeric values like int.\n",
        "here rangeBetween is applied on transaction_date which is in date format.\n",
        "so convert it to numeric.\n",
        "unix_timestamp converts into int"
      ],
      "metadata": {
        "id": "Cz-NusLHVyFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"transaction_date\",unix_timestamp(col(\"transaction_date\")))\n",
        "df.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "59g8hyIpMkxY",
        "outputId": "750f36a7-2b93-4b18-ad7b-f24182893e2d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------+\n",
            "|user_id|transaction_date|revenue|\n",
            "+-------+----------------+-------+\n",
            "|      1|      1717200000|    100|\n",
            "|      1|      1717286400|    200|\n",
            "|      1|      1717372800|    300|\n",
            "|      1|      1717545600|    400|\n",
            "|      2|      1717200000|    150|\n",
            "|      2|      1717459200|    250|\n",
            "|      2|      1717545600|    350|\n",
            "+-------+----------------+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, transaction_date: bigint, revenue: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Use +1 if you want to exclude the earliest boundary of your 3-day range.\n",
        "\n",
        "Don’t use +1 if you want a fully inclusive 3-day window."
      ],
      "metadata": {
        "id": "vFaZ3cCKY6ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_3lag = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\").rangeBetween(-3*86400 +1 ,0)\n",
        "df.withColumn(\" \",sum(\"revenue\").over(window_3lag)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdbTdDBlWnRi",
        "outputId": "76e85847-1be4-45b1-ff55-6a7775abf1f0"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------+---+\n",
            "|user_id|transaction_date|revenue|   |\n",
            "+-------+----------------+-------+---+\n",
            "|      1|      1717200000|    100|100|\n",
            "|      1|      1717286400|    200|300|\n",
            "|      1|      1717372800|    300|600|\n",
            "|      1|      1717545600|    400|700|\n",
            "|      2|      1717200000|    150|150|\n",
            "|      2|      1717459200|    250|250|\n",
            "|      2|      1717545600|    350|600|\n",
            "+-------+----------------+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PN60_81hXYuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}