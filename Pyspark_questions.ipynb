{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3Z3/G0phNTdBbQSRMOK9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/PySpark/blob/main/Pyspark_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:          \n",
        "Handling Different Delimiters in a Row\n",
        "You have a row with four columns, each delimited by a different delimiter (e.g., comma, SLT, pipe symbol). How would you handle this scenario in PySpark to split the row into columns based on the different delimiters?**"
      ],
      "metadata": {
        "id": "JXn3YvPBlzh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col"
      ],
      "metadata": {
        "id": "Dy19VktBo03a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .appName(\"Pyspark Questions\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "UvZz6D0TqBk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"1,Alice\\t30|New York\"]"
      ],
      "metadata": {
        "id": "kP0pkr25o1Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1df = spark.createDataFrame(data,'string')\n",
        "data1df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V3UN3uWo1X_",
        "outputId": "c09aeafe-ecc4-4933-ba23-39cdc453ef08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|1,Alice\\t30|New York|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_col = split(data1df['value'], ',|\\t|\\|')   # split_col will contain array with values"
      ],
      "metadata": {
        "id": "Y23JicUFrF7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split_col is array here\n",
        "fulldf = split_col.withColumn('id', split_col.getItem(0))\\\n",
        "       .withColumn('name', split_col.getItem(1))\\\n",
        "       .withColumn('age', split_col.getItem(2))\\\n",
        "       .withColumn('city', split_col.getItem(3))\n",
        "fulldf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "wePfkXsG2tmB",
        "outputId": "ca60ad3c-12a6-45a8-dc6f-08c8cdd10ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'getItem'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-98219caa8216>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'city'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'getItem'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1df = data1df.withColumn('id', split_col.getItem(0))\\\n",
        "                .withColumn('name',split_col.getItem(1))\\\n",
        "                .withColumn('age',split_col.getItem(2))\\\n",
        "                .withColumn('city',split_col.getItem(3))\n",
        "data1df.show()\n",
        "data1df.select('*').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHoEjtHuxtQD",
        "outputId": "e8f471e0-5308-4d36-9830-6d5453538a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---+-----+---+--------+\n",
            "|               value| id| name|age|    city|\n",
            "+--------------------+---+-----+---+--------+\n",
            "|1,Alice\\t30|New York|  1|Alice| 30|New York|\n",
            "+--------------------+---+-----+---+--------+\n",
            "\n",
            "+--------------------+---+-----+---+--------+\n",
            "|               value| id| name|age|    city|\n",
            "+--------------------+---+-----+---+--------+\n",
            "|1,Alice\\t30|New York|  1|Alice| 30|New York|\n",
            "+--------------------+---+-----+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1df.select(split_col.getItem(0).alias('id'),\n",
        "               split_col.getItem(1).alias('name'),\n",
        "               split_col.getItem(2).alias('age'),\n",
        "               split_col.getItem(3).alias('city')\n",
        "               ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0vcr_9v7i4B",
        "outputId": "f4d738ae-49aa-4a8e-ee72-38816bc94f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+--------+\n",
            "| id| name|age|    city|\n",
            "+---+-----+---+--------+\n",
            "|  1|Alice| 30|New York|\n",
            "+---+-----+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:            \n",
        "Identifying Missing Numbers in a List\n",
        "Given a list of numbers with some missing values (e.g., [1, 2, 4, 5, 7, 8, 10]), how would you identify the missing numbers (e.g., 3, 6, 9) using PySpark?**"
      ],
      "metadata": {
        "id": "9qo2Dw8wnaBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The error occurs because spark.createDataFrame() cannot infer a schema directly from a list of integers. You need to convert the list into a list of tuples or specify the schema explicitly.\n",
        "data = [1,2,3,4,5,7,8,10]\n",
        "df2 = spark.createDataFrame(data,[\"Number\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "jMA0lC0krE1V",
        "outputId": "d3ac3d2f-be31-4868-b69c-37a099dca79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-4f46e0f07193>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Number\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#datatype is Numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         schema = reduce(\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             (\n\u001b[0;32m--> 958\u001b[0;31m                 _infer_schema(\n\u001b[0m\u001b[1;32m    959\u001b[0m                     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m                     \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1685\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_SCHEMA_FOR_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(num,) for num in [1,2,3,4,5,7,8,10]]  #[(1,), (2,), (3,), (4,), (5,), (7,), (8,), (10,)]\n",
        "df2 = spark.createDataFrame(data,[\"Number\"])  #number is column name\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp2o6lDJAJvJ",
        "outputId": "b579114b-1535-40c9-ed5b-af1d84319356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Number|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     3|\n",
            "|     4|\n",
            "|     5|\n",
            "|     7|\n",
            "|     8|\n",
            "|    10|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new dataframe consisting of all values\n",
        "full_range = spark.range(1,11).toDF('Number')\n",
        "full_range.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA9L_XY7BODt",
        "outputId": "c3ca9e27-4969-4f01-fbd8-7119437ba65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Number|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     3|\n",
            "|     4|\n",
            "|     5|\n",
            "|     6|\n",
            "|     7|\n",
            "|     8|\n",
            "|     9|\n",
            "|    10|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now use left anti join to retrive the missing values in left table\n",
        "full_range.join(df2,\"Number\",\"left_anti\").show() #missing values in right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAhO7wbCC1HQ",
        "outputId": "d7b551cb-0ca6-48a9-8195-ef4ecdfa6a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Number|\n",
            "+------+\n",
            "|     6|\n",
            "|     9|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:                     Finding Top 3 Movies Based on Ratings\n",
        "You have two datasets: one with movie details (movie ID and movie name) and another with user ratings (movie ID, user ID, and rating). How would you find the top 3 movies based on their average ratings using PySpark?**"
      ],
      "metadata": {
        "id": "dgVrcfGynaWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "data_movies = [(1, \"Movie A\"), (2, \"Movie B\"), (3, \"Movie C\"), (4, \"Movie D\"), (5, \"Movie E\")]\n",
        "\n",
        "data_ratings = [(1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0),\n",
        "                (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0),\n",
        "                (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
        "#schema\n",
        "columns_movies = [\"MovieID\", \"MovieName\"]\n",
        "columns_ratings = [\"MovieID\", \"UserID\", \"Rating\"]\n",
        "\n",
        "#dataframes\n",
        "movies_df = spark.createDataFrame(data_movies, columns_movies)\n",
        "movies_df.show()\n",
        "ratings_df = spark.createDataFrame(data_ratings, columns_ratings)\n",
        "ratings_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtnsQXxYES5I",
        "outputId": "d2388e7c-1e1b-4946-b251-33ad04812c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|MovieID|MovieName|\n",
            "+-------+---------+\n",
            "|      1|  Movie A|\n",
            "|      2|  Movie B|\n",
            "|      3|  Movie C|\n",
            "|      4|  Movie D|\n",
            "|      5|  Movie E|\n",
            "+-------+---------+\n",
            "\n",
            "+-------+------+------+\n",
            "|MovieID|UserID|Rating|\n",
            "+-------+------+------+\n",
            "|      1|   101|   4.5|\n",
            "|      1|   102|   4.0|\n",
            "|      2|   103|   5.0|\n",
            "|      2|   104|   3.5|\n",
            "|      3|   105|   4.0|\n",
            "|      3|   106|   4.0|\n",
            "|      4|   107|   3.0|\n",
            "|      5|   108|   2.5|\n",
            "|      5|   109|   3.0|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "avg_rate = ratings_df.select(\"MovieID\",\"Rating\").groupBy('MovieID').agg(avg('Rating').alias('AvgRate'))\n",
        "avg_rate.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Gu-4GsFhA-",
        "outputId": "fa970bdb-caf4-4f27-c984-f98e893f65d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|MovieID|AvgRate|\n",
            "+-------+-------+\n",
            "|      1|   4.25|\n",
            "|      2|   4.25|\n",
            "|      5|   2.75|\n",
            "|      3|    4.0|\n",
            "|      4|    3.0|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "avg_rate.join(movies_df,\"MovieID\",'left').orderBy(desc('AvgRate')).limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHfG3NJHHvTW",
        "outputId": "67ff3f61-6eda-458a-bea8-df0982117d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+---------+\n",
            "|MovieID|AvgRate|MovieName|\n",
            "+-------+-------+---------+\n",
            "|      1|   4.25|  Movie A|\n",
            "|      2|   4.25|  Movie B|\n",
            "|      3|    4.0|  Movie C|\n",
            "+-------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df.select('*').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjUUqoGPSliT",
        "outputId": "51107466-0e63-4554-892d-f13e9b68c140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|MovieID|UserID|Rating|\n",
            "+-------+------+------+\n",
            "|      1|   101|   4.5|\n",
            "|      1|   102|   4.0|\n",
            "|      2|   103|   5.0|\n",
            "|      2|   104|   3.5|\n",
            "|      3|   105|   4.0|\n",
            "|      3|   106|   4.0|\n",
            "|      4|   107|   3.0|\n",
            "|      5|   108|   2.5|\n",
            "|      5|   109|   3.0|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, desc\n",
        "avg_rate = ratings_df.groupBy('MovieID')\\\n",
        "                    .agg(avg('Rating').alias(\"AvgRating\"))\\\n",
        "                    .orderBy(desc(\"AvgRating\"))\\\n",
        "                    .limit(3)\n",
        "avg_rate.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqW3_0VcTQQ0",
        "outputId": "96d56f97-0afb-48a5-c309-629556647d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|MovieID|AvgRating|\n",
            "+-------+---------+\n",
            "|      1|     4.25|\n",
            "|      2|     4.25|\n",
            "|      3|      4.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rate.join(movies_df,\"MovieID\",\"left\")\\\n",
        "        .select('MovieName',\"AvgRating\")\\\n",
        "        .orderBy(desc('AvgRating'))\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAh_-pKqUd07",
        "outputId": "94a495fc-1fa6-4bbf-b9e8-10667bca0c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|MovieName|AvgRating|\n",
            "+---------+---------+\n",
            "|  Movie A|     4.25|\n",
            "|  Movie B|     4.25|\n",
            "|  Movie C|      4.0|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Calculating a 7-Day Rolling Average\n",
        "Given a sales dataset with columns date, product ID, and quantity sold, how would you calculate a 7-day rolling average of the quantity sold for each product using PySpark?**"
      ],
      "metadata": {
        "id": "ZMu3OleUnasj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import Row\n",
        "\n",
        "#data\n",
        "data = [Row(Date='2023-01-01', ProductID=100, QuantitySold=10),\n",
        "        Row(Date='2023-01-02', ProductID=100, QuantitySold=15),\n",
        "        Row(Date='2023-01-03', ProductID=100, QuantitySold=20),\n",
        "        Row(Date='2023-01-04', ProductID=100, QuantitySold=25),\n",
        "        Row(Date='2023-01-05', ProductID=100, QuantitySold=30),\n",
        "        Row(Date='2023-01-06', ProductID=100, QuantitySold=35),\n",
        "        Row(Date='2023-01-07', ProductID=100, QuantitySold=40),\n",
        "        Row(Date='2023-01-08', ProductID=100, QuantitySold=45)]\n",
        "\n",
        "#dataframe\n",
        "df4 = spark.createDataFrame(data)\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIxMfGtMW77V",
        "outputId": "918292b9-0510-445b-88c4-24db3f6eb43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+\n",
            "|      Date|ProductID|QuantitySold|\n",
            "+----------+---------+------------+\n",
            "|2023-01-01|      100|          10|\n",
            "|2023-01-02|      100|          15|\n",
            "|2023-01-03|      100|          20|\n",
            "|2023-01-04|      100|          25|\n",
            "|2023-01-05|      100|          30|\n",
            "|2023-01-06|      100|          35|\n",
            "|2023-01-07|      100|          40|\n",
            "|2023-01-08|      100|          45|\n",
            "+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3lxhLGMbU5i",
        "outputId": "9175332d-5cfe-438b-9000-63ed3c235600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: string, ProductID: bigint, QuantitySold: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df4.withColumn(\"Date\", col(\"Date\").cast(\"date\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vuehb0ia9g9",
        "outputId": "5e8d4f05-5dba-4a9e-f58b-20af490a5f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, ProductID: bigint, QuantitySold: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting string data\n",
        "from pyspark.sql.functions import to_date\n",
        "df4 = df4.withColumn(\"Date\", to_date(\"Date\",\"yyyy-MM-dd\"))\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lrsOHWaZahe",
        "outputId": "e3893e66-145a-417a-9ad4-dcd5184cdaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+\n",
            "|      Date|ProductID|QuantitySold|\n",
            "+----------+---------+------------+\n",
            "|2023-01-01|      100|          10|\n",
            "|2023-01-02|      100|          15|\n",
            "|2023-01-03|      100|          20|\n",
            "|2023-01-04|      100|          25|\n",
            "|2023-01-05|      100|          30|\n",
            "|2023-01-06|      100|          35|\n",
            "|2023-01-07|      100|          40|\n",
            "|2023-01-08|      100|          45|\n",
            "+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "#window\n",
        "windowdf1 = Window.partitionBy(\"ProductID\")\\\n",
        "                  .rowsBetween(-6,0)\n",
        "#apply the window created\n",
        "df4.withColumn(\"7Day_Qua_Avg\", avg(df4.QuantitySold).over(windowdf1)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB1mM1oJbbxC",
        "outputId": "e2f3274b-c62a-45c4-fa29-1a3e5bd12e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+------------+\n",
            "|      Date|ProductID|QuantitySold|7Day_Qua_Avg|\n",
            "+----------+---------+------------+------------+\n",
            "|2023-01-01|      100|          10|        10.0|\n",
            "|2023-01-02|      100|          15|        12.5|\n",
            "|2023-01-03|      100|          20|        15.0|\n",
            "|2023-01-04|      100|          25|        17.5|\n",
            "|2023-01-05|      100|          30|        20.0|\n",
            "|2023-01-06|      100|          35|        22.5|\n",
            "|2023-01-07|      100|          40|        25.0|\n",
            "|2023-01-08|      100|          45|        30.0|\n",
            "+----------+---------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import\n",
        "df4.select('*',\n",
        "           PartitionB\n",
        "           )"
      ],
      "metadata": {
        "id": "_AePM9DeYPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Using UDFs to Categorize Ages\n",
        "How would you use a User-Defined Function (UDF) in PySpark to categorize ages into groups like \"youth,\" \"adult,\" and \"senior\"?**"
      ],
      "metadata": {
        "id": "sF4Pk9J8na_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "data = [Row(UserID=4001, Age=17),\n",
        "        Row(UserID=4002, Age=45),\n",
        "        Row(UserID=4003, Age=65),\n",
        "        Row(UserID=4004, Age=30),\n",
        "        Row(UserID=4005, Age=80)]\n",
        "\n",
        "df5 = spark.createDataFrame(data)\n",
        "df5.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsyHqu6wnbCX",
        "outputId": "b9ef6c3e-e141-460c-c7b5-181f45eb1663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|UserID|Age|\n",
            "+------+---+\n",
            "|  4001| 17|\n",
            "|  4002| 45|\n",
            "|  4003| 65|\n",
            "|  4004| 30|\n",
            "|  4005| 80|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "# function\n",
        "def age_cat(age):\n",
        "  if age < 18:\n",
        "    return 'Youth'\n",
        "  elif age < 60:\n",
        "    return 'Adult'\n",
        "  else:\n",
        "    return 'Senior'\n",
        "\n",
        "age_udf = udf(age_cat,StringType())\n",
        "df5.withColumn(\"AgeGroup\",age_udf(col(\"Age\"))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOhETkbzn8ri",
        "outputId": "4738ecfc-1a6c-4638-b273-d5f3a26fc20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------+\n",
            "|UserID|Age|AgeGroup|\n",
            "+------+---+--------+\n",
            "|  4001| 17|   Youth|\n",
            "|  4002| 45|   Adult|\n",
            "|  4003| 65|  Senior|\n",
            "|  4004| 30|   Adult|\n",
            "|  4005| 80|  Senior|\n",
            "+------+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:                   Counting Unique Website Visitors per Day\n",
        "Given a dataset with date and visitor ID, how would you calculate the count of unique visitors to a website per day using PySpark?**"
      ],
      "metadata": {
        "id": "wDuw2B_Ync-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "visitor_data = [Row(Date='2023-01-01', VisitorID=101),\n",
        "                Row(Date='2023-01-01', VisitorID=102),\n",
        "                Row(Date='2023-01-01', VisitorID=101),\n",
        "                Row(Date='2023-01-02', VisitorID=103),\n",
        "                Row(Date='2023-01-02', VisitorID=101)]\n",
        "\n",
        "df6 = spark.createDataFrame(visitor_data)\n",
        "df6.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujcmx2k4tNiI",
        "outputId": "cd66f5de-fa89-422a-c9ae-faad2abad07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|      Date|VisitorID|\n",
            "+----------+---------+\n",
            "|2023-01-01|      101|\n",
            "|2023-01-01|      102|\n",
            "|2023-01-01|      101|\n",
            "|2023-01-02|      103|\n",
            "|2023-01-02|      101|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "df6.groupBy(\"Date\").agg(countDistinct(\"VisitorID\").alias(\"Count\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0-mNTqbt7dt",
        "outputId": "303674f3-2d8e-40dc-8f77-afa4c1007158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|      Date|Count|\n",
            "+----------+-----+\n",
            "|2023-01-01|    2|\n",
            "|2023-01-02|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Finding the First Purchase Date of Each User\n",
        "Given a dataset with user ID and purchase date, how would you determine the first purchase date of each user using PySpark?**"
      ],
      "metadata": {
        "id": "cSnqG3qfndMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "purchase_data = [\n",
        "    Row(UserID=1, PurchaseDate='2023-01-05'),\n",
        "    Row(UserID=1, PurchaseDate='2023-01-10'),\n",
        "    Row(UserID=2, PurchaseDate='2023-01-03'),\n",
        "    Row(UserID=3, PurchaseDate='2023-01-12')\n",
        "]\n",
        "df7 = spark.createDataFrame(purchase_data)\n",
        "df7.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VvU4EdyvW-C",
        "outputId": "40be9b7a-4144-4c68-d74c-a66cddeed0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|UserID|PurchaseDate|\n",
            "+------+------------+\n",
            "|     1|  2023-01-05|\n",
            "|     1|  2023-01-10|\n",
            "|     2|  2023-01-03|\n",
            "|     3|  2023-01-12|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df7.withColumn(\"PurchaseDate\", to_date(col(\"PurchaseDate\"))).show()\n",
        "df7.withColumn(\"PurchaseDate\",col(\"PurchaseDate\").cast(\"date\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phidCfx8xE9m",
        "outputId": "d78005be-a00a-4dfc-a858-081813c5079f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|UserID|PurchaseDate|\n",
            "+------+------------+\n",
            "|     1|  2023-01-05|\n",
            "|     1|  2023-01-10|\n",
            "|     2|  2023-01-03|\n",
            "|     3|  2023-01-12|\n",
            "+------+------------+\n",
            "\n",
            "+------+------------+\n",
            "|UserID|PurchaseDate|\n",
            "+------+------------+\n",
            "|     1|  2023-01-05|\n",
            "|     1|  2023-01-10|\n",
            "|     2|  2023-01-03|\n",
            "|     3|  2023-01-12|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min\n",
        "df7.groupBy(\"UserID\").agg(min(\"PurchaseDate\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RwCZrH-wNEr",
        "outputId": "118461c2-6c0c-48d9-f7a7-709626596cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|UserID|min(PurchaseDate)|\n",
            "+------+-----------------+\n",
            "|     1|       2023-01-05|\n",
            "|     2|       2023-01-03|\n",
            "|     3|       2023-01-12|\n",
            "+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Generating Sequential Numbers Within Groups\n",
        "Given a dataset with group ID and date, how would you generate a sequential number for each row within each group, ordered by date, using PySpark?**"
      ],
      "metadata": {
        "id": "dhgTDO4dndgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "group_data = [\n",
        "    Row(GroupID='A', Date='2023-01-01'),\n",
        "    Row(GroupID='A', Date='2023-01-02'),\n",
        "    Row(GroupID='B', Date='2023-01-01'),\n",
        "    Row(GroupID='B', Date='2023-01-03')\n",
        "]\n",
        "\n",
        "df8 = spark.createDataFrame(group_data)\n",
        "df8.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryHzl0syx_la",
        "outputId": "4627e00f-bb02-4e82-9009-191f9bbf63e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|GroupID|      Date|\n",
            "+-------+----------+\n",
            "|      A|2023-01-01|\n",
            "|      A|2023-01-02|\n",
            "|      B|2023-01-01|\n",
            "|      B|2023-01-03|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# window\n",
        "windowdf8 = Window.partitionBy(\"GroupID\")\\\n",
        "            .orderBy(\"Date\")\n",
        "\n",
        "df8.select(\"*\", row_number().over(windowdf8).alias(\"Rank\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTKHfz7ykIZ",
        "outputId": "f4ad367f-942e-42c7-9d46-16ebeeeee44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+\n",
            "|GroupID|      Date|Rank|\n",
            "+-------+----------+----+\n",
            "|      A|2023-01-01|   1|\n",
            "|      A|2023-01-02|   2|\n",
            "|      B|2023-01-01|   1|\n",
            "|      B|2023-01-03|   2|\n",
            "+-------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Replacing Null Values with the Mean\n",
        "Given a dataset with sales ID and amount (some of which are null), how would you replace the null values with the mean of the amount column using PySpark?**"
      ],
      "metadata": {
        "id": "ttG5ZvrVndwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "sales_data = [(\"1\", 100), (\"2\", 150), (\"3\", None), (\"4\", 200), (\"5\", None)]\n",
        "schema8 = [\"sale_id\", \"amount\"]\n",
        "df9 = spark.createDataFrame(sales_data,schema8)\n",
        "df9.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoMe3JKC2luT",
        "outputId": "cd33421d-d2cc-49c0-90f6-a57e09657d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|sale_id|amount|\n",
            "+-------+------+\n",
            "|      1|   100|\n",
            "|      2|   150|\n",
            "|      3|  NULL|\n",
            "|      4|   200|\n",
            "|      5|  NULL|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  pyspark.sql.functions import mean\n",
        "mean_val = df9.agg(mean(\"amount\"))\n",
        "mean_val.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNSzZaf43lve",
        "outputId": "69c0f7de-df63-4909-859b-c4cd2ebce4e1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|avg(amount)|\n",
            "+-----------+\n",
            "|      150.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we have to convert this datarame to value.\n",
        "# so we extrat from dataframe using collect()\n",
        "mean_val = df9.agg(mean(\"amount\")).collect()[0][0] # firstrow,firstcol all list is computed\n",
        "mean_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcA5mYn5jehK",
        "outputId": "69c1f743-0f12-48bd-8524-07d75d75d5fd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150.0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_val = df9.agg(mean(\"amount\")).first()[0] # entire list not computed only first\n",
        "mean_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gywq4xZoj7zU",
        "outputId": "a5392b6e-b80e-42e3-e053-b113d43c9957"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150.0"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df9.na.fill(mean_val,[\"amount\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgmdaJbGHwTe",
        "outputId": "a6acc137-0171-44a9-85df-1dce183ff270"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|sale_id|amount|\n",
            "+-------+------+\n",
            "|      1|   100|\n",
            "|      2|   150|\n",
            "|      3|   150|\n",
            "|      4|   200|\n",
            "|      5|   150|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Reshaping Data Using Pivot\n",
        "Given a dataset of monthly sales per product, how would you reshape the data to have one row per product-month combination using PySpark?**"
      ],
      "metadata": {
        "id": "vDswMq1fneAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "data = [(\"Product1\", 100, 150, 200),\n",
        "        (\"Product2\", 200, 250, 300),\n",
        "        (\"Product3\", 300, 350, 400)]\n",
        "columns = [\"Product\", \"Sales_Jan\", \"Sales_Feb\", \"Sales_Mar\"]\n",
        "df10 = spark.createDataFrame(data, columns)\n",
        "df10.show()"
      ],
      "metadata": {
        "id": "6yb-mtMBm_5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b210ead-3ec0-418a-cf63-054f07112c9d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+---------+---------+\n",
            "| Product|Sales_Jan|Sales_Feb|Sales_Mar|\n",
            "+--------+---------+---------+---------+\n",
            "|Product1|      100|      150|      200|\n",
            "|Product2|      200|      250|      300|\n",
            "|Product3|      300|      350|      400|\n",
            "+--------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pivoted_data = df10.selectExpr( \"Product\",\n",
        "                               'stack(3,\"Jan\", Sales_Jan ,\"Feb\", Sales_Feb, \"Mar\",Sales_Mar) as (Month, Sale)'\n",
        "                                )\n",
        "pivoted_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CNdTaHIl0no",
        "outputId": "55009d7d-06d1-457b-aeed-3e80e90b8a2c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+----+\n",
            "| Product|Month|Sale|\n",
            "+--------+-----+----+\n",
            "|Product1|  Jan| 100|\n",
            "|Product1|  Feb| 150|\n",
            "|Product1|  Mar| 200|\n",
            "|Product2|  Jan| 200|\n",
            "|Product2|  Feb| 250|\n",
            "|Product2|  Mar| 300|\n",
            "|Product3|  Jan| 300|\n",
            "|Product3|  Feb| 350|\n",
            "|Product3|  Mar| 400|\n",
            "+--------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df10.selectExpr(\"Product\",\"Sales_Jan as Jan\",\"Sales_Feb as Feb\",\"Sales_Mar as Mar\")\\\n",
        "          .melt([\"Product\"],[\"Sales_Jan\",\"Sales_Feb\",\"Sales_Mar\"],\"Month\",\"Sales\").show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "Jx71igeMpt1X",
        "outputId": "7306b173-5f71-47dd-d55a-5496bb7071ec"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Sales_Jan` cannot be resolved. Did you mean one of the following? [`Jan`, `Feb`, `Mar`, `Product`].;\n'Unpivot ArraySeq(Product#742), ArraySeq(List('Sales_Jan), List('Sales_Feb), List('Sales_Mar)), Month, [Sales]\n+- Project [Product#742, Sales_Jan#743L AS Jan#788L, Sales_Feb#744L AS Feb#789L, Sales_Mar#745L AS Mar#790L]\n   +- LogicalRDD [Product#742, Sales_Jan#743L, Sales_Feb#744L, Sales_Mar#745L], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-d5acf41c966d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Product\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales_Jan as Jan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales_Feb as Feb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales_Mar as Mar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m           \u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Product\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sales_Jan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales_Feb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales_Mar\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sales\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(self, ids, values, variableColumnName, valueColumnName)\u001b[0m\n\u001b[1;32m   3663\u001b[0m         \u001b[0mSupports\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mConnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3664\u001b[0m         \"\"\"\n\u001b[0;32m-> 3665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariableColumnName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueColumnName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3667\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munpivot\u001b[0;34m(self, ids, values, variableColumnName, valueColumnName)\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3617\u001b[0m             \u001b[0mjvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3618\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpivotWithSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariableColumnName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueColumnName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Sales_Jan` cannot be resolved. Did you mean one of the following? [`Jan`, `Feb`, `Mar`, `Product`].;\n'Unpivot ArraySeq(Product#742), ArraySeq(List('Sales_Jan), List('Sales_Feb), List('Sales_Mar)), Month, [Sales]\n+- Project [Product#742, Sales_Jan#743L AS Jan#788L, Sales_Feb#744L AS Feb#789L, Sales_Mar#745L AS Mar#790L]\n   +- LogicalRDD [Product#742, Sales_Jan#743L, Sales_Feb#744L, Sales_Mar#745L], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11: Write a pyspark code to get top 10 most frequently used words in a text file ingnoring the words like 'the','a'.**"
      ],
      "metadata": {
        "id": "zaAECkl3vFx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data11 = [(\"Python Program to Reverse a Number Last Updated : 26 Feb, 2025 We are given a number and our task is to reverse its digits. \"\n",
        "     \"For example, if the input is 12345 then the output should be 54321. In this article, we will explore various techniques for \"\n",
        "     \"reversing a number in Python. Using String Slicing In this example, the Python code reverses a given number by converting it \"\n",
        "     \"to a string, slicing it in reverse order and then converting it back to an integer. The original and reversed numbers are \"\n",
        "     \"printed for the example case where the original number is 1234.\",)]\n",
        "\n",
        "column11 = [\"value\"]\n",
        "\n",
        "df11 = spark.createDataFrame(data11,column11)\n",
        "df11.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DaQ_9LuNGN",
        "outputId": "49613eee-c365-4e30-83bd-6022ec8c22ca"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|Python Program to...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = {\"the\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\", \"on\", \"is\", \"it\", \"for\", \"with\", \"as\", \"by\"}\n"
      ],
      "metadata": {
        "id": "BLv183UTxYHv"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, lower, regexp_replace,explode\n",
        "df11.select(explode(split(lower(regexp_replace(col('value'),'[^a-zA-Z]','')),' ')).alias(\"Word\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ3fyJMUyD0f",
        "outputId": "6b926430-ae9d-42de-a4b8-dea018745b6d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                Word|\n",
            "+--------------------+\n",
            "|pythonprogramtore...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmdIWFczzJe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}